{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f266bcaa",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"images/nhsa_logo.png\" alt=\"Image\" style=\"text-align: left;\">\n",
    "</div>\n",
    "\n",
    "# Parent Gauge Data Analysis Project\n",
    "---\n",
    "## Data Wrangling Script and Documentation\n",
    "\n",
    "In this script, we will provide a step-by-step demonstration of how script is being cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada7c4d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "#uses old version of google trans: pip3 install googletrans==3.1.0a0\n",
    "from googletrans import Translator\n",
    "\n",
    "# Utility function for gender imputation - DISCLAIMER: Sensitivity and Accuracy Considerations\n",
    "# This function is intended for filling in missing gender values for statistical purposes only.\n",
    "# Please note that gender imputation methods may not accurately reflect an individual's gender identity.\n",
    "# Use caution and sensitivity when interpreting or applying these imputed values.\n",
    "import gender_guesser.detector as gender_guesser\n",
    "from genderize import Genderize\n",
    "import sexmachine.detector as sexmachine\n",
    "\n",
    "#utilize a Named Entity Recognition (NER) library to detect and remove named entities like names\n",
    "import spacy\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e3169b-daad-48f6-81b5-7b2f4e85e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data into the dataframe\n",
    "df = pd.read_excel('../data/INTVDATA.xlsx', sheet_name ='Main', engine ='openpyxl')\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880529ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the new .csv file\n",
    "df = pd.read_csv('../data/intv_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b43f5",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb847af-89c4-4385-b576-a29fd0615f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Drop Duplicate Rows\n",
    "\n",
    "A majority of the duplicate rows in the parent gauge dataset involve rows that are duplicates of the header row. We will use a random variable such as 'date' and remove rows that equal the name of the variable. Afterwards, we will drop additional duplicates, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05f7da0-19b2-4c92-bfef-9e340735cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "#To make sure all duplicate \"header\" rows are eliminated, we pick a random column,\n",
    "#remove rows where 'date' column is equal to 'date', except the first row\n",
    "# Initialize a counter for deleted rows\n",
    "deleted_date_rows = 0\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    if row['date'] == 'date':\n",
    "        # Remove the row if the date matches\n",
    "        df.drop(i, inplace=True)\n",
    "        deleted_date_rows += 1\n",
    "\n",
    "# Count the number of rows before dropping duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Drop duplicate rows based on all columns except the first two, since they are indexed\n",
    "result = df.drop_duplicates(subset=df.columns[2:])\n",
    "        \n",
    "# Count the number of rows after dropping duplicates\n",
    "rows_after = len(result)\n",
    "\n",
    "# Calculate and print the number of rows dropped\n",
    "rows_dropped = (rows_before - rows_after) + deleted_date_rows\n",
    "print(f\"Number of duplicate rows dropped: {rows_dropped}\")\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aefeac-e571-4881-8a0a-ab1a8b21ad62",
   "metadata": {},
   "source": [
    "## Sample Data Frame Generation\n",
    "\n",
    "Note: You can run this code again if you would like to reset the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdede69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a sample of 10% of the total dataset\n"
     ]
    }
   ],
   "source": [
    "#Because the main dataset is too large for data cleaning, \n",
    "#construct a small sample for faster processing. Once I am done coding, we will use the entire dataset.\n",
    "df_sample = df.sample(frac=0.1)\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n",
    "print(\"Created a sample of 10% of the total dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc37ee6",
   "metadata": {},
   "source": [
    "## Main Data Cleaning\n",
    "\n",
    "This is a summary of all the data cleaning and reformatting steps that were conducted.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067236d-26f6-4dcf-98b9-e9b68751588b",
   "metadata": {},
   "source": [
    "# Code to Clean\n",
    "\n",
    "This section examines each variable and transforms and cleans them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77e40-2bab-49b2-b109-695cd4b10243",
   "metadata": {},
   "source": [
    "## Center\n",
    "\n",
    "For the center, we want to correspond each center to their respective state and geographic location for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36f9bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_centers = df_sample['center'].unique().tolist()\n",
    "\n",
    "# Sort the list in place\n",
    "unique_centers.sort()\n",
    "\n",
    "#create a text file of the unique programs\n",
    "with open('../data/unique_centers.txt', 'w') as f:\n",
    "    for item in unique_centers:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9231c-5aba-4b41-b54f-5b91242b2d42",
   "metadata": {},
   "source": [
    "## Created_at\n",
    " drop this feature, as it is unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04603ece-23cd-4ff9-aa5d-daee20bde4de",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "Here, we will break down the date into three columns, seperating the year, month, and day. This code performs data cleaning on a column called 'date' in the dataset. It first checks each entry in the column for any errors in the date format, printing an error message if any are found. Then, it converts the 'date' column to a standard datetime format and creates new columns for the year, month, and day extracted from the dates. Finally, the cleaned data is saved to a CSV file. This code ensures that the dates are properly formatted, allows for easy analysis based on different time periods, and provides a clean dataset for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e79c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the entries in the 'date' column\n",
    "for i, date in enumerate(df_sample['date']):\n",
    "    try:\n",
    "        # Try to convert the date to datetime format\n",
    "        pd.to_datetime(date, format='mixed')\n",
    "    except Exception:\n",
    "        print(f\"An error occurred at index {i} with the date: {date}\")\n",
    "        \n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "#Create separate 'year', 'month', and 'day' columns\n",
    "df_sample['date_year'] = df_sample['date'].dt.year\n",
    "df_sample['date_month'] = df_sample['date'].dt.month\n",
    "df_sample['date_day'] = df_sample['date'].dt.day\n",
    "\n",
    "#save to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dea179-7f4f-4d83-8aea-1a15e8aabb91",
   "metadata": {},
   "source": [
    "## Evaluation Year\n",
    "\n",
    "The original format of the evaluation year was formatted as '2016-2017', for instance. For easier analysis, the start and end year have been split up into two columns, \"evaluation_start_year\" and \"evaluation_end_year.\" Moreover, we convert the new variables into a faster data type. Lastly, try to fix any erroneous years, then remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70434815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Split the 'evaluation_year' column into two separate columns 'start_year' and 'end_year'\n",
    "df_sample[['evaluation_start_year', 'evaluation_end_year']] = df_sample['evaluation_year'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 'evaluation_start_year' and 'evaluation_end_year to int16\n",
    "df_sample['evaluation_start_year'] = df_sample['evaluation_start_year'].astype('int16')\n",
    "df_sample['evaluation_end_year'] = df_sample['evaluation_end_year'].astype('int16')\n",
    "\n",
    "# Convert 'student_enrollment_date' column to datetime, in preparation for referencing\n",
    "df_sample['student_enrollment_date'] = pd.to_datetime(df_sample['student_enrollment_date'])\n",
    "\n",
    "# NOTE: Parent guage officially started in 2017, so years far from that range are erroneous.\n",
    "# FIX errneous rows that were supposed to be a valid year\n",
    "# Store the indices of rows with updated evaluation_start_year\n",
    "updated_indices = df_sample[(df_sample['evaluation_start_year'] < 2016) | (df_sample['evaluation_start_year'] > current_year)].index\n",
    "\n",
    "# Update evaluation_start_year based on evaluation and student_enrollment_date, which is less erroneous\n",
    "df_sample.loc[df_sample['evaluation_start_year'] < 2016, 'evaluation_start_year'] = df_sample.loc[df_sample['evaluation_start_year'] < 2016].apply(\n",
    "    lambda row: row['student_enrollment_date'].year if row['evaluation'] == 'Initial' else row['student_enrollment_date'].year + 1, axis=1\n",
    ")\n",
    "\n",
    "df_sample.loc[df_sample['evaluation_start_year'] > current_year, 'evaluation_start_year'] = df_sample.loc[df_sample['evaluation_start_year'] > current_year].apply(\n",
    "    lambda row: row['student_enrollment_date'].year if row['evaluation'] == 'Initial' else row['student_enrollment_date'].year + 1, axis=1\n",
    ")\n",
    "\n",
    "# Update evaluation_end_year only for the updated rows\n",
    "df_sample.loc[updated_indices, 'evaluation_end_year'] = df_sample.loc[updated_indices, 'evaluation_start_year'] + 1\n",
    "\n",
    "# Clear erroneous values where 'evaluation_start_year' is below 2016 or above current_year\n",
    "df_sample.loc[df_sample['evaluation_start_year'] < 2016, ['evaluation_year', 'evaluation_start_year', 'evaluation_end_year']] = None\n",
    "df_sample.loc[df_sample['evaluation_start_year'] > current_year, ['evaluation_year', 'evaluation_start_year', 'evaluation_end_year']] = None\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3600a-9e0d-45bb-8b0d-dabedd5d447b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dummy variables have been created, breaking the three categorical variables into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb51bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one-hot encoding to create dummy variables in preparation for regression.\n",
    "#WARNING: This, however eliminates the original 'evaluation' column\n",
    "df_sample = pd.get_dummies(df_sample, columns=['evaluation'])\n",
    "\n",
    "#drop excess 'evaluation_evaluation' column, if it exists.\n",
    "if 'evaluation_evaluation' in df_sample.columns:\n",
    "    df_sample.drop('evaluation_evaluation', axis=1, inplace=True)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91907a-6497-4597-b2fa-51833440742e",
   "metadata": {},
   "source": [
    "## Guardian Name\n",
    "\n",
    "After employment the name should be scrubbed for privacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3562b-8532-4949-9da0-715f7d47dd6e",
   "metadata": {},
   "source": [
    "## Guardian Employment\n",
    "\n",
    "upon further inspection, it seems that the guardian employment is tied to the guardian_id, and it is not possible to further fill in missing values. note that over 60% of guardian_employment is missing. We should consider dropping this variable and removing it from our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ec11-d8cb-4c4b-8708-8ad9f2afa55e",
   "metadata": {},
   "source": [
    "## Guardian Enrollment Date\n",
    "\n",
    "Use this variable to determine how many years the parent has been in the parent gauge program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60356e33-2ece-47e9-9fec-f3b3621ccc1a",
   "metadata": {},
   "source": [
    "## Guardian Highest Education\n",
    "\n",
    "same as guardian_employment, no referencing of the guardian_id will fill in missing blanks. Note that over 60% of guardian employment is misssing. We should consider dropping this variable and removing it from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91308fcc-6658-46b1-8b79-e0d99dea5e2d",
   "metadata": {},
   "source": [
    "## Guardian, Hispanic\n",
    "\n",
    "There are missing values, but many other variables including the guardian's native language, student's native language, whether the student is hispanic, and what language the interview was used—to determine whether the guardian was hispanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cf8fc10-fd5d-4859-84e6-ea606898c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "#reconcile \"dont know\" and \"refused\" values\n",
    "####pending...\n",
    "\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"student_hispanic\", \n",
    "#\"student_native_language\", \"language of interview\", \n",
    "# Define a function to fill missing values in guardian_hispanic column based on conditions\n",
    "def fill_guardian_hispanic(row):\n",
    "    if pd.isnull(row['guardian_hispanic']) or row['guardian_hispanic'] not in ['Yes', 'No']:\n",
    "        if row['student_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['student_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    else:\n",
    "        return row['guardian_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in guardian_hispanic column\n",
    "df_sample['guardian_hispanic'] = df_sample.apply(fill_guardian_hispanic, axis=1)\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "df_sample['guardian_hispanic'] = df_sample['guardian_hispanic'].map({'Yes': True, 'No': False})\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6765367-b42f-4132-a26e-d852345480de",
   "metadata": {},
   "source": [
    "## Guardian Native Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a92fbe-0191-4258-a1a7-5d4fa4b0e2f8",
   "metadata": {},
   "source": [
    "## Guardian Race\n",
    "\n",
    "tasks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d4bfd93-d8c2-4f83-8854-2f10c0c0bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Value  Count\n",
      "0                                               White  10156\n",
      "1                           Black or African American   5171\n",
      "2                                                 NaN   1615\n",
      "3                                               Other   1484\n",
      "4                                        Multi-Racial    712\n",
      "5                                          Don't Know    383\n",
      "6                                               Asian    374\n",
      "7                    American Indian or Alaska Native    200\n",
      "8                                             , White    190\n",
      "9                                         Unspecified     37\n",
      "10                Native Hawaiian or Pacific Islander     37\n",
      "11                        , Black or African American     20\n",
      "12                                     , Multi-Racial     13\n",
      "13                                            Refused     10\n",
      "14                                White, Multi-Racial      8\n",
      "15                                       White, Other      8\n",
      "16                   White, Black or African American      7\n",
      "17            Black or African American, Multi-Racial      6\n",
      "18                                       Other, White      6\n",
      "19                                            , Other      6\n",
      "20                                       White, Asian      4\n",
      "21                   Black or African American, White      4\n",
      "22  Native Hawaiian or Pacific Islander, Black or ...      2\n",
      "23                   Black or African American, Other      2\n",
      "24                                     , Other, White      1\n",
      "25                 , American Indian or Alaska Native      1\n",
      "26          Native Hawaiian or Other Pacific Islander      1\n",
      "27  Native Hawaiian or Pacific Islander, American ...      1\n",
      "28                                Other, Multi-Racial      1\n",
      "29            Multi-Racial, Black or African American      1\n",
      "30                                     White, Refused      1\n",
      "31                   Other, Black or African American      1\n",
      "32            American Indian or Alaska Native, Other      1\n",
      "33                   Asian, Black or African American      1\n",
      "34                              , White, Multi-Racial      1\n",
      "35                                           [object]      1\n",
      "36                                  Don't Know, White      1\n"
     ]
    }
   ],
   "source": [
    "table_breakdown = df_sample['guardian_race'].value_counts(dropna=False).reset_index()\n",
    "table_breakdown.columns = ['Value', 'Count']\n",
    "\n",
    "print(table_breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201ede-9c7f-4763-9d8c-d792f76f4a1e",
   "metadata": {},
   "source": [
    "## Guardian Date of Birth\n",
    "\n",
    "use guardian's DOB and interview year to determine guardian’s age during time of interview. We create a new variable named 'guardian_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48824aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use guardian's DOB and interview year to determine guardian’s age during time of interview. \n",
    "# Convert 'guardian_birth_date' and 'date' columns to datetime\n",
    "df_sample['guardian_birth_date'] = pd.to_datetime(df_sample['guardian_birth_date'], errors='coerce')\n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "# Calculate the age at the time of the interview\n",
    "df_sample['guardian_age'] = pd.NaT  # Initialize the column with missing values\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['guardian_birth_date']).days // 365\n",
    "        df_sample.at[i, 'guardian_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "# Remove rows where 'guardian_age' values are less than 18 or above 99\n",
    "df_sample = df_sample[(df_sample['guardian_age'] >= 18) & (df_sample['guardian_age'] <= 99)]\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b7e9-9333-4e1c-b5e2-7a5285052490",
   "metadata": {},
   "source": [
    "## Guardian Sex\n",
    "\n",
    "Here, we use the gender guesser to fill in missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6879ab75-8f49-4803-9bfa-3a5a31f5b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean some values for more proper analysis\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('F', 'Female')\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('M', 'Male')\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('Unknown', '')\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9680dbf4-ac28-4897-a3e2-174f71d51789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both the \"gender_guesser\", \"Genderize\", and \"sexmachine\" library to fill in missing values\n",
    "# warning issue: running too many libraries in the function creates bottlenecks. \n",
    "def guess_gender(name):\n",
    "    # Using sexmachine library\n",
    "    try:\n",
    "        d_sexmachine = sexmachine.Detector(case_sensitive=False)\n",
    "        genders_sexmachine = [d_sexmachine.get_gender(name) for name in names]\n",
    "\n",
    "        for name, gender_sexmachine in zip(names, genders_sexmachine):\n",
    "            if gender_sexmachine == 'male':\n",
    "                results.append((name, 'Male'))\n",
    "            elif gender_sexmachine == 'female':\n",
    "                results.append((name, 'Female'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Using genderize library\n",
    "    try:\n",
    "        genderize_results = Genderize().get(names)\n",
    "        for name, genderize_result in zip(names, genderize_results):\n",
    "            if 'gender' in genderize_result and genderize_result['gender'] is not None:\n",
    "                results.append((name, genderize_result['gender'].capitalize()))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Using gender_guesser library\n",
    "    d_guesser = gender_guesser.Detector()\n",
    "    gender_guess = d_guesser.get_gender(name)\n",
    "\n",
    "    if gender_guess == 'male' or gender_guess == 'mostly_male':\n",
    "        return 'Male'\n",
    "    elif gender_guess == 'female' or gender_guess == 'mostly_female':\n",
    "        return 'Female'\n",
    "\n",
    "    return None\n",
    "\n",
    "df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == '' else row['guardian_sex'], axis=1)\n",
    "df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if pd.isnull(row['guardian_sex']) else row['guardian_sex'], axis=1)\n",
    "df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == 'None' else row['guardian_sex'], axis=1)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d9c77c0-73d9-4d0d-82bf-c6175123631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Value  Count\n",
      "0  Female  18031\n",
      "1    Male   1585\n",
      "2    None    435\n",
      "3   Other      3\n"
     ]
    }
   ],
   "source": [
    "table_breakdown = df_sample['guardian_sex'].value_counts(dropna=False).reset_index()\n",
    "table_breakdown.columns = ['Value', 'Count']\n",
    "\n",
    "print(table_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab4ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using direct mapping to create dummy variable out of guardian_sex\n",
    "df_sample['female'] = np.where(df_sample['guardian_sex'] == 'Female', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c051d4-09b8-4a7f-ac1e-99568cf937c0",
   "metadata": {},
   "source": [
    "## Guardian Vendor ID\n",
    "\n",
    "consider dropping this feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ff1a3-2d2b-45fe-a196-43733ee1493b",
   "metadata": {},
   "source": [
    "## Interview ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45a88d-0e72-49c2-93db-ceb3e40e2f43",
   "metadata": {},
   "source": [
    "## Interviewer Name\n",
    "\n",
    "Drop for privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e7015-af4b-4f45-839a-aaa8126e0974",
   "metadata": {},
   "source": [
    "## Interviewer ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571618f-6aa2-49a7-b219-4598690a8a2d",
   "metadata": {},
   "source": [
    "## Interviewer Vendor ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bb666-c052-4c9a-a373-7a2638ddf708",
   "metadata": {},
   "source": [
    "## Language of Interview\n",
    "\n",
    "There are a few missing or invalid variables. As most interviews were done in the english language, impute variable with \"English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53330583-3c56-4a9d-bcee-19727360cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume interview was done in english. Fill empty values with \"English\"\n",
    "df_sample['language'].fillna(\"English\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820c9b2-5124-4242-9cfd-7c0c568ff999",
   "metadata": {},
   "source": [
    "## Mode of Interview\n",
    "\n",
    "convert to dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c772ac3-3273-4e95-94a5-f2fafb0efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dummy variable\n",
    "# Using direct mapping to create dummy variable out of mode\n",
    "df_sample['phone_interview'] = np.where(df_sample['mode'] == 'Phone', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca48e3-011c-40fa-80af-e53e44b0d3b0",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d4b94-f760-4f48-8314-2e352a8a0ee9",
   "metadata": {},
   "source": [
    "## Student Name\n",
    "\n",
    "drop the name for privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d460a-4967-4f05-ae00-2384c66eb7ed",
   "metadata": {},
   "source": [
    "## Student Enrollment Date\n",
    "\n",
    "Erroneous years range from 1915 to 2121. need to fix them, and reference other columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e901115-b373-42f8-b4c7-fdedbe29d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'student_enrollment_date' to datetime format\n",
    "df['student_enrollment_date'] = pd.to_datetime(df['student_enrollment_date'], errors='coerce')\n",
    "\n",
    "#fix erroneous and missing years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db8661-9234-4a9e-8585-81f63f07fd08",
   "metadata": {},
   "source": [
    "## Student Disability Status\n",
    "\n",
    "*If entry is blank, change to 'None' if interview disregards questions intended for parents of students with disabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723b93ec-c71e-40f9-b34a-a980b54da879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert disability questions from string to numeric data type\n",
    "df_sample['QD1'] = pd.to_numeric(df_sample['QD1'], errors='coerce')\n",
    "df_sample['QD1a'] = pd.to_numeric(df_sample['QD1a'], errors='coerce')\n",
    "df_sample['QD2'] = pd.to_numeric(df_sample['QD2'], errors='coerce')\n",
    "df_sample['QD2a'] = pd.to_numeric(df_sample['QD2a'], errors='coerce')\n",
    "\n",
    "# Custom function to check if any of the disability question columns have value between 1 to 5\n",
    "def has_disability(row):\n",
    "    return 1 <= row['QD1'] <= 5 or 1 <= row['QD1a'] <= 5 or 1 <= row['QD2'] <= 5 or 1 <= row['QD2a'] <= 5\n",
    "\n",
    "# Apply the custom function and fill in the missing values in 'student_has_disability'\n",
    "df_sample['student_has_disability'] = df_sample.apply(lambda row: has_disability(row) if pd.isna(row['student_has_disability']) else row['student_has_disability'], axis=1)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6f308-0d54-4e57-a7df-9144cee7c5dd",
   "metadata": {},
   "source": [
    "## Student, Hispanic\n",
    "\n",
    "remove anything that is not \"yes\" or \"no\". reconcile \"dont know\" and \"refused\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd96b756-0128-4ce0-976d-9ed748f4de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "#reconcile \"dont know\" and \"refused\" values\n",
    "\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"guardian_hispanic\", \"student_native_language\"\n",
    "# Define a function to fill missing values in student_hispanic column based on conditions\n",
    "def fill_student_hispanic(row):\n",
    "    if pd.isnull(row['student_hispanic']):\n",
    "        if row['guardian_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['guardian_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif pd.notnull(row['student_native_language']):\n",
    "            return 'No'\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row['student_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in student_hispanic column\n",
    "df_sample['student_hispanic'] = df_sample.apply(fill_student_hispanic, axis=1)\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "#df_sample = pd.get_dummies(df_sample, columns=[\"guardian_hispanic\"], prefix=\"guardian_hispanic\", drop_first=True)\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b218699-95ec-4e6b-a7ae-74b31d48bb5e",
   "metadata": {},
   "source": [
    "## Student, ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d3621-4b56-45a5-8328-971053cb8350",
   "metadata": {},
   "source": [
    "## Student Birth Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3992624e-2eef-49a3-8936-f7fe0c932c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use student's DOB and interview year to determine student’s age during time of interview. \n",
    "# Convert 'student_birth_date' and 'date' columns to datetime\n",
    "df_sample['student_birth_date'] = pd.to_datetime(df_sample['student_birth_date'], errors='coerce')\n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "# Calculate the age at the time of the guardian's interview\n",
    "df_sample['student_age'] = pd.NaT  # Initialize the column with missing values\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['student_birth_date']).days // 365\n",
    "        df_sample.at[i, 'student_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "# Remove rows where 'student_age' values are less than -1 or above 6\n",
    "df_sample = df_sample[(df_sample['student_age'] >= -1) & (df_sample['student_age'] <= 6)]\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b75e0-4753-4a15-8d2d-068d65477886",
   "metadata": {},
   "source": [
    "## Student in last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd7fc8a-e7ca-4ad4-9ceb-993567a9ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how do you fill up all these missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5fbc1-5ea4-4de7-ad32-6a3c9b995b47",
   "metadata": {},
   "source": [
    "## Student Native Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacffd9-07cf-4768-abfb-b3623d10c85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941a46cb-a2b7-4d1b-91b4-c6f5ca63f617",
   "metadata": {},
   "source": [
    "## Student Program Type\n",
    "\n",
    "For missing values, we can check the center's program type. Then, need to deal with variables that state both \"head start\" and \"early head start.\" Can possibly also base it off student age. 7% is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa036dcc-1861-4468-be6d-0c0931daa523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d19ee73-5275-450c-bddd-98476604d5dc",
   "metadata": {},
   "source": [
    "## Student Race\n",
    "\n",
    "#there are too many combinations, over 85 of them. need to simplify and create dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb8856-0702-4e8c-9eed-aba58ec89b13",
   "metadata": {},
   "source": [
    "## Student Service Type\n",
    "\n",
    "Try to reference off missing variables like program or center. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a8e69-c4e4-4d25-8c87-cc9ce23bd330",
   "metadata": {},
   "source": [
    "## Student Sex\n",
    "\n",
    "use gender_guesser again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "268dbcb7-8e4a-43ec-9a1d-03b44d6f5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_gender(name):\n",
    "    d = gender.Detector()\n",
    "    gender_guess = d.get_gender(name)\n",
    "    return gender_guess\n",
    "\n",
    "# Apply guess_gender function to fill missing values in 'student_sex' column\n",
    "df_sample['student_sex'] = df_sample.apply(lambda row: guess_gender(row['student']) if pd.isnull(row['student_sex']) else row['student_sex'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fc8f9-47e2-4a3f-9ca2-24d4a9609aa1",
   "metadata": {},
   "source": [
    "## Student Staff\n",
    "\n",
    "drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82383d2-47ab-4d3f-80e5-93fbdc3153af",
   "metadata": {},
   "source": [
    "## Student Staff ID\n",
    "\n",
    "drop this column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341642f-841e-4452-ba8a-2c893685dfe3",
   "metadata": {},
   "source": [
    "## Student Staff Vendor ID\n",
    "\n",
    "consider removing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0c85e-4433-49e5-9525-61515f73a7e0",
   "metadata": {},
   "source": [
    "## Student Vendor ID\n",
    "\n",
    "remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82471932-cc4a-45ec-8ee2-5a62ff5b2c9c",
   "metadata": {},
   "source": [
    "## Student Was Early Headstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91c4368a-4264-48dd-b630-c59d2b757a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of 'True' and 'False' to boolean\n",
    "df_sample['student_was_early_head_start'] = df_sample['student_was_early_head_start'].replace({'False': False, 'True': True})\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef1b7-0e2a-4ab1-a3b4-1771c58c7509",
   "metadata": {},
   "source": [
    "## Student Was Head Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e8dfe52-a0b2-4f0f-bf57-ff13298cd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of 'True' and 'False' to boolean\n",
    "df_sample['student_was_head_start'] = df_sample['student_was_head_start'].replace({'False': False, 'True': True})\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece5bc2-d63b-41d6-97b7-d63da2e8f1db",
   "metadata": {},
   "source": [
    "# Next Section: Likert Scale Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb678305-1a6e-4640-9a30-eb12157d53ed",
   "metadata": {},
   "source": [
    "### CHANGE ALL UNANSWERED VALUES TO BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d1a6db2-7144-4713-a59e-bc2444e61ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##CHANGE ALL UNANSWERED VALUES TO BLANK##\n",
    "#determine all of the likert scale interview variables\n",
    "\n",
    "variables = ['Q1', 'Q1a', 'Q1b', 'Q1c', 'Q1d', 'Q2', 'Q2a', 'Q3', 'Q3a', 'Q4', 'Q4a', 'Q5', 'Q5a', 'Q6', 'Q6a',\n",
    "             'Q7', 'Q7a', 'Q8', 'Q8a', 'QD1', 'QD1a', 'QD2', 'QD2a', 'Q9', 'Q9a', 'Q10', 'Q10a', 'Q11', 'Q11a',\n",
    "             'Q12', 'Q12a', 'Q13', 'Q13a', 'Q14', 'Q14a', 'Q15', 'Q15a', 'Q16', 'Q16a', 'Q17', 'Q17a', 'Q18',\n",
    "             'Q18a', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25']\n",
    "\n",
    "#convert string values to numeric, change the datatype to int\n",
    "df_sample[variables] = df_sample[variables].apply(pd.to_numeric, errors='coerce', downcast='integer')\n",
    "\n",
    "#replace the row values with blanks if the value is below 0\n",
    "df_sample[variables] = df_sample[variables].applymap(lambda x: '' if x < 0 else x)\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad311e54-93f8-4baa-a8eb-49d1cf8059f5",
   "metadata": {},
   "source": [
    "### DROP ALL BLANK INTERVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ae61458-8512-4a9b-9943-ae76dfdbe516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows deleted where all 'Q' variables are blank: 2087\n",
      "Percentage of rows deleted where all 'Q' variables are blank: 10.20%\n"
     ]
    }
   ],
   "source": [
    "# Select columns starting with 'Q'\n",
    "q_columns = [col for col in df_sample.columns if col.startswith('Q')]\n",
    "\n",
    "# Count the number of rows where all 'Q' columns are blank or null\n",
    "count_all_blank = len(df_sample[(df_sample[q_columns].isnull() | (df_sample[q_columns] == \"\")).all(axis=1)])\n",
    "\n",
    "# Calculate the percentage of rows where all 'Q' columns are blank\n",
    "percentage_all_blank = (count_all_blank / len(df_sample)) * 100\n",
    "\n",
    "# Delete the rows where all 'Q' columns are blank\n",
    "df_sample = df_sample[~(df_sample[q_columns].isnull() | (df_sample[q_columns] == \"\")).all(axis=1)]\n",
    "\n",
    "# Print the number and percentage of deleted rows\n",
    "print(\"Number of rows deleted where all 'Q' variables are blank:\", count_all_blank)\n",
    "print(\"Percentage of rows deleted where all 'Q' variables are blank: {:.2f}%\".format(percentage_all_blank))\n",
    "\n",
    "# Write the updated dataframe to .csv file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27ba4bcc-e989-4f27-8ae9-79301c5ccdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing values in 'Q8a': 529\n",
      "Percentage of missing values in 'Q8a': 3.88%\n"
     ]
    }
   ],
   "source": [
    "## need to consider how questions are intended to not be answered based on evaluation period and circumstances (e.g. student disability), filter them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6279c-22f0-417d-b643-47474cefcc2e",
   "metadata": {},
   "source": [
    "# Next Section: Open Interview Questions\n",
    "\n",
    "perhaps we can use data analysis to see how sentiments change\n",
    "https://www.surveypractice.org/article/25699-what-to-do-with-all-those-open-ended-responses-data-visualization-techniques-for-survey-researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8da15bc-743b-4ac1-b0a3-71c64338d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first need to translate spanish-language to english\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50ecf159-4630-4cbf-9766-fc3d13f12643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         YES I DO THE STORY THAT I HAVE SINCE MY OLD CH...\n",
      "1                                                       nan\n",
      "2         I am more open with telling the staff any prob...\n",
      "3         it has helped me and my children . I have lear...\n",
      "4         I mean my relationship with Stacy , its a good...\n",
      "                                ...                        \n",
      "204670    This is our last summer with the program .   W...\n",
      "204671    Mantuvo conversaciones , escucho acerca de nue...\n",
      "204672    La maestra hizo preguntas acerca de mi familia...\n",
      "204673    Se intereso por mi familia y nuestras necesida...\n",
      "204674    Manteniendo conversaciones , haciendo pregunta...\n",
      "Name: OQ1_cleaned, Length: 204675, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#SCRUB ALL NAMES WITH GENERIC NAMES\n",
    "# Load the spaCy multilingual language model\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# Define the generic placeholder for student names\n",
    "generic_name = \"Student\"\n",
    "\n",
    "# Define a function to remove names from a text string\n",
    "def remove_names(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = ' '.join([token.text if token.ent_type_ != 1 else generic_name for token in doc])\n",
    "    return cleaned_text\n",
    "\n",
    "# Convert float values to strings in the 'OQ1' column\n",
    "df['OQ1'] = df['OQ1'].astype(str)\n",
    "\n",
    "# Apply the function to the 'OQ1' column\n",
    "df['OQ1_cleaned'] = df['OQ1'].apply(remove_names)\n",
    "\n",
    "# Print the updated column\n",
    "print(df['OQ1_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10a045-101d-4eac-b9b7-5ab05cc725c1",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed86eab9-d6bf-4dee-8277-6121a18d4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower() #lowercases\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) #removes special characters and punctuation\n",
    "    text = text.strip() #normalizes text\n",
    "    # Apply additional normalization steps as needed\n",
    "    return text\n",
    "\n",
    "for column in df.columns:\n",
    "    if column.startswith('OQ'):\n",
    "        df_sample[column] = df_sample[column].apply(normalize_text)\n",
    "        \n",
    "\n",
    "#TOKENIZATION\n",
    "#REMOVING STOPWORDS\n",
    "#CORRECTING MISSPELLED WORDS\n",
    "#STEMMING AND LEMATIZATION\n",
    "#HANDLE MISSING VALUES\n",
    "\n",
    "# Write the updated dataframe to .csv file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39607fda-6bc8-4493-a872-cd304150ac36",
   "metadata": {},
   "source": [
    "# Remove Unnecessary Columns\n",
    "\n",
    "After meticulously going through the data cleaning process, it is important to ensure that the dataset is streamlined and efficient for analysis. Removing excess columns that do not contribute to the analysis or could introduce noise is a critical step in data preparation. This step not only enhances the readability and manageability of the dataset but also optimizes memory usage and potentially speeds up processing time. It’s vital to scrutinize each column and ascertain its relevance in context to the analysis goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f5a74-e6d5-4ad7-adc1-cbfe26822685",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE UNNECESSARY AND NO LONGER NEEDED COLUMNS\n",
    "# List of columns to be removed\n",
    "columns_to_remove = ['created_at', 'guardian', 'guardian_employment', 'guardian_highest_education', 'guardian_birth_date', 'evaluation_year'\n",
    "                     'guardian_vendor_id', 'interview_id', 'interviewer_id', 'mode',\n",
    "                     'interviewer', 'interviewer_vendor_id', 'student', 'student_staff_vendor_id', 'student_birth_date'\n",
    "                     'student_vendor_id', 'student_staff', 'student_staff_id']\n",
    "\n",
    "# Removing the columns from the DataFrame\n",
    "df_sample = df_sample.drop(columns=columns_to_remove)\n",
    "\n",
    "#save updates to working csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb83f76-53e3-4a61-ad2e-ba6bdfa8a8a4",
   "metadata": {},
   "source": [
    "## Data Type Reconfiguration\n",
    "\n",
    "To make the analysis faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400611cb-c46c-4f7b-b1e2-5568d0923ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30e0a0e1-e852-47b1-83dc-578fbb6822e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'program': 223\n"
     ]
    }
   ],
   "source": [
    "unique_values = df['program'].nunique()\n",
    "print(f\"Number of unique values in 'program': {unique_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
