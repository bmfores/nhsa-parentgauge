{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f266bcaa",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"images/nhsa_logo.png\" alt=\"Image\" style=\"text-align: left;\">\n",
    "</div>\n",
    "\n",
    "# Parent Gauge Data Analysis Project\n",
    "---\n",
    "## Data Wrangling Script and Documentation\n",
    "\n",
    "In this script, we will provide a step-by-step demonstration of how script is being cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada7c4d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "#uses old version of google trans: pip3 install googletrans==3.1.0a0\n",
    "from googletrans import Translator\n",
    "\n",
    "#used for filling in missing genders --DISCLAIMER: I recognize the sensitivities of this matter and understand \n",
    "#that this may not be totally accurate of the individual. This will only be used for statistical purposes.\n",
    "import gender_guesser.detector as gender_guesser\n",
    "from genderize import Genderize\n",
    "import sexmachine.detector as sexmachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880529ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_66155/428991487.py:9: DtypeWarning: Columns (13,18,20,30,38,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/intv_data.csv')\n"
     ]
    }
   ],
   "source": [
    "#Load the Data into the dataframe\n",
    "df = pd.read_excel('../data/INTVDATA.xlsx', sheet_name ='Main', engine ='openpyxl')\n",
    "######MAKE SURE TO SPECIFY DATATYPE LATER ON######\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)\n",
    "\n",
    "#read the new .csv file\n",
    "df = pd.read_csv('../data/intv_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b43f5",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb847af-89c4-4385-b576-a29fd0615f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Drop Duplicate Rows\n",
    "\n",
    "A majority of the duplicate rows in the parent gauge dataset involve rows that are duplicates of the header row. We will use a random variable such as 'date' and remove rows that equal the name of the variable. Afterwards, we will drop additional duplicates, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05f7da0-19b2-4c92-bfef-9e340735cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows dropped: 204\n"
     ]
    }
   ],
   "source": [
    "#To make sure all duplicate \"header\" rows are eliminated, we pick a random column,\n",
    "#remove rows where 'date' column is equal to 'date', except the first row\n",
    "# Initialize a counter for deleted rows\n",
    "deleted_date_rows = 0\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    if row['date'] == 'date':\n",
    "        # Remove the row if the date matches\n",
    "        df.drop(i, inplace=True)\n",
    "        deleted_date_rows += 1\n",
    "\n",
    "# Count the number of rows before dropping duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Drop duplicate rows based on all columns except the first two, since they are indexed\n",
    "result = df.drop_duplicates(subset=df.columns[2:])\n",
    "        \n",
    "# Count the number of rows after dropping duplicates\n",
    "rows_after = len(result)\n",
    "\n",
    "# Calculate and print the number of rows dropped\n",
    "rows_dropped = (rows_before - rows_after) + deleted_date_rows\n",
    "print(f\"Number of duplicate rows dropped: {rows_dropped}\")\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aefeac-e571-4881-8a0a-ab1a8b21ad62",
   "metadata": {},
   "source": [
    "## Sample Data Frame Generation\n",
    "\n",
    "Note: You can run this code again if you would like to reset the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdede69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a sample of 10% of the total dataset\n"
     ]
    }
   ],
   "source": [
    "#Because the main dataset is too large for data cleaning, \n",
    "#construct a small sample for faster processing. Once I am done coding, we will use the entire dataset.\n",
    "df_sample = df.sample(frac=0.1)\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n",
    "print(\"Created a sample of 10% of the total dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc37ee6",
   "metadata": {},
   "source": [
    "## Main Data Cleaning\n",
    "\n",
    "This is a summary of all the data cleaning and reformatting steps that were conducted.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067236d-26f6-4dcf-98b9-e9b68751588b",
   "metadata": {},
   "source": [
    "# Code to Clean\n",
    "\n",
    "This section examines each variable and transforms and cleans them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77e40-2bab-49b2-b109-695cd4b10243",
   "metadata": {},
   "source": [
    "## Center\n",
    "\n",
    "For the center, we want to correspond each center to their respective state and geographic location for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36f9bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_centers = df_sample['center'].unique().tolist()\n",
    "\n",
    "# Sort the list in place\n",
    "unique_centers.sort()\n",
    "\n",
    "#create a text file of the unique programs\n",
    "with open('../data/unique_centers.txt', 'w') as f:\n",
    "    for item in unique_centers:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9231c-5aba-4b41-b54f-5b91242b2d42",
   "metadata": {},
   "source": [
    "## Created_at\n",
    " drop this feature, as it is unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04603ece-23cd-4ff9-aa5d-daee20bde4de",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "Here, we will break down the date into three columns, seperating the year, month, and day. This code performs data cleaning on a column called 'date' in the dataset. It first checks each entry in the column for any errors in the date format, printing an error message if any are found. Then, it converts the 'date' column to a standard datetime format and creates new columns for the year, month, and day extracted from the dates. Finally, the cleaned data is saved to a CSV file. This code ensures that the dates are properly formatted, allows for easy analysis based on different time periods, and provides a clean dataset for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e79c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the entries in the 'date' column\n",
    "for i, date in enumerate(df_sample['date']):\n",
    "    try:\n",
    "        # Try to convert the date to datetime format\n",
    "        pd.to_datetime(date, format='mixed')\n",
    "    except Exception:\n",
    "        print(f\"An error occurred at index {i} with the date: {date}\")\n",
    "        \n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "#Create separate 'year', 'month', and 'day' columns\n",
    "df_sample['date_year'] = df_sample['date'].dt.year\n",
    "df_sample['date_month'] = df_sample['date'].dt.month\n",
    "df_sample['date_day'] = df_sample['date'].dt.day\n",
    "\n",
    "#save to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3600a-9e0d-45bb-8b0d-dabedd5d447b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dummy variables have been created, breaking the three categorical variables into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb51bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one-hot encoding to create dummy variables in preparation for regression.\n",
    "#WARNING: This, however eliminates the original 'evaluation' column\n",
    "df_sample = pd.get_dummies(df_sample, columns=['evaluation'])\n",
    "\n",
    "#drop excess 'evaluation_evaluation' column, if it exists.\n",
    "if 'evaluation_evaluation' in df_sample.columns:\n",
    "    df_sample.drop('evaluation_evaluation', axis=1, inplace=True)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dea179-7f4f-4d83-8aea-1a15e8aabb91",
   "metadata": {},
   "source": [
    "## Evaluation Year\n",
    "\n",
    "The original format of the evaluation year was formatted as '2016-2017', for instance. For easier analysis, the start and end year have been split up into two columns, \"evaluation_start_year\" and \"evaluation_end_year.\" Moreover, we convert the new variables into a faster data type. Lastly, try to fix any erroneous years, then remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70434815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'evaluation_year' column into two separate columns 'start_year' and 'end_year'\n",
    "df_sample[['evaluation_start_year', 'evaluation_end_year']] = df_sample['evaluation_year'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 'evaluation_start_year' and 'evaluation_end_year to int16\n",
    "df_sample['evaluation_start_year'] = df_sample['evaluation_start_year'].astype('int16')\n",
    "df_sample['evaluation_end_year'] = df_sample['evaluation_end_year'].astype('int16')\n",
    "\n",
    "# Fix errneous rows that were supposed to be a valid year\n",
    "#####################\n",
    "\n",
    "# Remove remaining erroneous rows where the 'evaluation_start_year' values are below 2015 or above 2023\n",
    "df_sample = df_sample[(df_sample['evaluation_start_year'] >= 2015) & (df_sample['evaluation_start_year'] <= 2023)]\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91907a-6497-4597-b2fa-51833440742e",
   "metadata": {},
   "source": [
    "## Guardian Name\n",
    "\n",
    "After employment the name should be scrubbed for privacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3562b-8532-4949-9da0-715f7d47dd6e",
   "metadata": {},
   "source": [
    "## Guardian Employment\n",
    "\n",
    "upon further inspection, it seems that the guardian employment is tied to the guardian_id, and it is not possible to further fill in missing values. note that over 60% of guardian_employment is missing. We should consider dropping this variable and removing it from our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ec11-d8cb-4c4b-8708-8ad9f2afa55e",
   "metadata": {},
   "source": [
    "## Guardian Enrollment Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60356e33-2ece-47e9-9fec-f3b3621ccc1a",
   "metadata": {},
   "source": [
    "## Guardian Highest Education\n",
    "\n",
    "same as guardian_employment, no referencing of the guardian_id will fill in missing blanks. Note that over 60% of guardian employment is misssing. We should consider dropping this variable and removing it from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91308fcc-6658-46b1-8b79-e0d99dea5e2d",
   "metadata": {},
   "source": [
    "## Guardian, Hispanic?\n",
    "\n",
    "There are missing values, but many other variables including the guardian's native language, student's native language, whether the student is hispanic, and what language the interview was used—to determine whether the guardian was hispanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cf8fc10-fd5d-4859-84e6-ea606898c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"student_hispanic\", \n",
    "#\"student_native_language\", \"language of interview\", \n",
    "# Define a function to fill missing values in guardian_hispanic column based on conditions\n",
    "def fill_guardian_hispanic(row):\n",
    "    if pd.isnull(row['guardian_hispanic']) or row['guardian_hispanic'] not in ['Yes', 'No']:\n",
    "        if row['student_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['student_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    else:\n",
    "        return row['guardian_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in guardian_hispanic column\n",
    "df_sample['guardian_hispanic'] = df_sample.apply(fill_guardian_hispanic, axis=1)\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "df_sample['guardian_hispanic'] = df_sample['guardian_hispanic'].map({'Yes': True, 'No': False})\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6765367-b42f-4132-a26e-d852345480de",
   "metadata": {},
   "source": [
    "## Guardian Native Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a92fbe-0191-4258-a1a7-5d4fa4b0e2f8",
   "metadata": {},
   "source": [
    "## Guardian Race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201ede-9c7f-4763-9d8c-d792f76f4a1e",
   "metadata": {},
   "source": [
    "## Guardian Date of Birth\n",
    "\n",
    "use guardian's DOB and interview year to determine guardian’s age during time of interview. We create a new variable named 'guardian_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48824aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use guardian's DOB and interview year to determine guardian’s age during time of interview. \n",
    "# Convert 'guardian_birth_date' and 'date' columns to datetime\n",
    "df_sample['guardian_birth_date'] = pd.to_datetime(df_sample['guardian_birth_date'], errors='coerce')\n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "# Calculate the age at the time of the interview\n",
    "df_sample['guardian_age'] = pd.NaT  # Initialize the column with missing values\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['guardian_birth_date']).days // 365\n",
    "        df_sample.at[i, 'guardian_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "# Remove rows where 'guardian_age' values are less than 18 or above 99\n",
    "df_sample = df_sample[(df_sample['guardian_age'] >= 18) & (df_sample['guardian_age'] <= 99)]\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b7e9-9333-4e1c-b5e2-7a5285052490",
   "metadata": {},
   "source": [
    "## Guardian Sex\n",
    "\n",
    "Here, we use the gender guesser to fill in missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6879ab75-8f49-4803-9bfa-3a5a31f5b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean some values for more proper analysis\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('F', 'Female')\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('M', 'Male')\n",
    "df_sample['guardian_sex'] = df_sample['guardian_sex'].replace('unknown', '')\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9680dbf4-ac28-4897-a3e2-174f71d51789",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply guess_gender function to fill missing values in 'guardian_sex' column\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguardian_sex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mguess_gender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian_sex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian_sex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == '' else row['guardian_sex'], axis=1)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == 'unknown' else row['guardian_sex'], axis=1)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#save updates to csv\u001b[39;00m\n\u001b[1;32m     34\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/sample_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply guess_gender function to fill missing values in 'guardian_sex' column\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguardian_sex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_sample\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mguess_gender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguardian_sex\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguardian_sex\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == '' else row['guardian_sex'], axis=1)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == 'unknown' else row['guardian_sex'], axis=1)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#save updates to csv\u001b[39;00m\n\u001b[1;32m     34\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/sample_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mguess_gender\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mguess_gender\u001b[39m(name):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Using gender_guesser library\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     d_guesser \u001b[38;5;241m=\u001b[39m \u001b[43mgender_guesser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     gender_guess \u001b[38;5;241m=\u001b[39m d_guesser\u001b[38;5;241m.\u001b[39mget_gender(name)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gender_guess \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmale\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m gender_guess \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmostly_male\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gender_guesser/detector.py:28\u001b[0m, in \u001b[0;36mDetector.__init__\u001b[0;34m(self, case_sensitive)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a detector parsing given data file\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcase_sensitive \u001b[38;5;241m=\u001b[39m case_sensitive\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/nam_dict.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gender_guesser/detector.py:34\u001b[0m, in \u001b[0;36mDetector._parse\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mopen(filename, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eat_name_line(line\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m<frozen codecs>:714\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:645\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:587\u001b[0m, in \u001b[0;36mreadline\u001b[0;34m(self, size, keepends)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use both the \"gender_guesser\", \"Genderize\", and \"sexmachine\" library to fill in missing values\n",
    "# warning issue: running too many libraries in the function creates bottlenecks. \n",
    "def guess_gender(name):\n",
    "    # Using gender_guesser library\n",
    "    d_guesser = gender_guesser.Detector()\n",
    "    gender_guess = d_guesser.get_gender(name)\n",
    "\n",
    "    if gender_guess == 'male' or gender_guess == 'mostly_male':\n",
    "        return 'Male'\n",
    "    elif gender_guess == 'female' or gender_guess == 'mostly_female':\n",
    "        return 'Female'\n",
    "\n",
    "    # Using sexmachine library\n",
    "    try:\n",
    "        d_sexmachine = sexmachine.Detector(case_sensitive=False)\n",
    "        genders_sexmachine = [d_sexmachine.get_gender(name) for name in names]\n",
    "\n",
    "        for name, gender_sexmachine in zip(names, genders_sexmachine):\n",
    "            if gender_sexmachine == 'male':\n",
    "                results.append((name, 'Male'))\n",
    "            elif gender_sexmachine == 'female':\n",
    "                results.append((name, 'Female'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply guess_gender function to fill missing values in 'guardian_sex' column\n",
    "df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if pd.isnull(row['guardian_sex']) else row['guardian_sex'], axis=1)\n",
    "#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == '' else row['guardian_sex'], axis=1)\n",
    "#df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if row['guardian_sex'] == 'unknown' else row['guardian_sex'], axis=1)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3d9c77c0-73d9-4d0d-82bf-c6175123631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Value  Count\n",
      "0   Female  18252\n",
      "1     Male   1588\n",
      "2      NaN    623\n",
      "3    Other      3\n",
      "4  Neutral      2\n"
     ]
    }
   ],
   "source": [
    "table_breakdown = df_sample['guardian_sex'].value_counts(dropna=False).reset_index()\n",
    "table_breakdown.columns = ['Value', 'Count']\n",
    "\n",
    "print(table_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab4ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using direct mapping to create dummy variable out of guardian_sex\n",
    "df_sample['female'] = np.where(df_sample['guardian_sex'] == 'Female', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c051d4-09b8-4a7f-ac1e-99568cf937c0",
   "metadata": {},
   "source": [
    "## Guardian Vendor ID\n",
    "\n",
    "consider dropping this feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ff1a3-2d2b-45fe-a196-43733ee1493b",
   "metadata": {},
   "source": [
    "## Interview ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45a88d-0e72-49c2-93db-ceb3e40e2f43",
   "metadata": {},
   "source": [
    "## Interviewer Name\n",
    "\n",
    "Drop for privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e7015-af4b-4f45-839a-aaa8126e0974",
   "metadata": {},
   "source": [
    "## Interviewer ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571618f-6aa2-49a7-b219-4598690a8a2d",
   "metadata": {},
   "source": [
    "## Interviewer Vendor ID\n",
    "\n",
    "drop this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bb666-c052-4c9a-a373-7a2638ddf708",
   "metadata": {},
   "source": [
    "## Language of Interview\n",
    "\n",
    "Here, (1) remove invalid languages i.e. \"language\". (2) next, use the guardian ID to fill in missing values. (3) Lastly consider removing or imputing rows that have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e633270e-e783-42de-9405-347f903ed843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: remove invalid languages i.e. language\n",
    "\n",
    "#step 2: use guardian_id to fill in missing values\n",
    "\n",
    "#use guardian_native_language\n",
    "\n",
    "#step 3: remove or impute rows that have missing values\n",
    "#do this by checking if any of the open interview responses were in a certain language, if they are, apply corresponding language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820c9b2-5124-4242-9cfd-7c0c568ff999",
   "metadata": {},
   "source": [
    "## Mode of Interview\n",
    "\n",
    "convert to dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c772ac3-3273-4e95-94a5-f2fafb0efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dummy variable\n",
    "# Using direct mapping to create dummy variable out of mode\n",
    "df_sample['phone_interview'] = np.where(df_sample['mode'] == 'Phone', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca48e3-011c-40fa-80af-e53e44b0d3b0",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d4b94-f760-4f48-8314-2e352a8a0ee9",
   "metadata": {},
   "source": [
    "## Student Name\n",
    "\n",
    "drop the name for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b6f7849-53a0-4d72-80fc-67e143d98eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d460a-4967-4f05-ae00-2384c66eb7ed",
   "metadata": {},
   "source": [
    "## Student Enrollment Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db8661-9234-4a9e-8585-81f63f07fd08",
   "metadata": {},
   "source": [
    "## Student Disability Status\n",
    "\n",
    "*If entry is blank, change to 'None' if interview disregards questions intended for parents of students with disabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723b93ec-c71e-40f9-b34a-a980b54da879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing rows in 'student_hispanic' column: 1136\n"
     ]
    }
   ],
   "source": [
    "# Count the number of missing rows in 'guardian_hispanic' column\n",
    "missing_count = df_sample['student_hispanic'].isnull().sum()\n",
    "\n",
    "# Print the number of missing rows\n",
    "print(\"Number of missing rows in 'student_hispanic' column:\", missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6f308-0d54-4e57-a7df-9144cee7c5dd",
   "metadata": {},
   "source": [
    "## Student, Hispanic\n",
    "##remove anything that is not \"yes\" or \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd96b756-0128-4ce0-976d-9ed748f4de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"guardian_hispanic\", \"student_native_language\"\n",
    "# Define a function to fill missing values in student_hispanic column based on conditions\n",
    "def fill_student_hispanic(row):\n",
    "    if pd.isnull(row['student_hispanic']):\n",
    "        if row['guardian_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['guardian_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif pd.notnull(row['student_native_language']):\n",
    "            return 'No'\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row['student_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in student_hispanic column\n",
    "df_sample['student_hispanic'] = df_sample.apply(fill_student_hispanic, axis=1)\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "#df_sample = pd.get_dummies(df_sample, columns=[\"guardian_hispanic\"], prefix=\"guardian_hispanic\", drop_first=True)\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b218699-95ec-4e6b-a7ae-74b31d48bb5e",
   "metadata": {},
   "source": [
    "## Student, ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d3621-4b56-45a5-8328-971053cb8350",
   "metadata": {},
   "source": [
    "## Student Birth Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3992624e-2eef-49a3-8936-f7fe0c932c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use student's DOB and interview year to determine student’s age during time of interview. \n",
    "# Convert 'student_birth_date' and 'date' columns to datetime\n",
    "df_sample['student_birth_date'] = pd.to_datetime(df_sample['student_birth_date'], errors='coerce')\n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "# Calculate the age at the time of the guardian's interview\n",
    "df_sample['student_age'] = pd.NaT  # Initialize the column with missing values\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['student_birth_date']).days // 365\n",
    "        df_sample.at[i, 'student_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "# Remove rows where 'student_age' values are less than -1 or above 6\n",
    "df_sample = df_sample[(df_sample['student_age'] >= -1) & (df_sample['student_age'] <= 6)]\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b75e0-4753-4a15-8d2d-068d65477886",
   "metadata": {},
   "source": [
    "## Student in last year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5fbc1-5ea4-4de7-ad32-6a3c9b995b47",
   "metadata": {},
   "source": [
    "## Student Native Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a46cb-a2b7-4d1b-91b4-c6f5ca63f617",
   "metadata": {},
   "source": [
    "## Student Program Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19ee73-5275-450c-bddd-98476604d5dc",
   "metadata": {},
   "source": [
    "## Student Race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb8856-0702-4e8c-9eed-aba58ec89b13",
   "metadata": {},
   "source": [
    "## Student Service Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a8e69-c4e4-4d25-8c87-cc9ce23bd330",
   "metadata": {},
   "source": [
    "## Student Sex\n",
    "\n",
    "use gender_guesser again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "268dbcb7-8e4a-43ec-9a1d-03b44d6f5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_gender(name):\n",
    "    d = gender.Detector()\n",
    "    gender_guess = d.get_gender(name)\n",
    "    return gender_guess\n",
    "\n",
    "# Apply guess_gender function to fill missing values in 'student_sex' column\n",
    "df_sample['student_sex'] = df_sample.apply(lambda row: guess_gender(row['student']) if pd.isnull(row['student_sex']) else row['student_sex'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fc8f9-47e2-4a3f-9ca2-24d4a9609aa1",
   "metadata": {},
   "source": [
    "## Student Staff\n",
    "\n",
    "drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82383d2-47ab-4d3f-80e5-93fbdc3153af",
   "metadata": {},
   "source": [
    "## Student Staff ID\n",
    "\n",
    "drop this column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341642f-841e-4452-ba8a-2c893685dfe3",
   "metadata": {},
   "source": [
    "## Student Staff Vendor ID\n",
    "\n",
    "consider removing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0c85e-4433-49e5-9525-61515f73a7e0",
   "metadata": {},
   "source": [
    "## Student Vendor ID\n",
    "\n",
    "remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82471932-cc4a-45ec-8ee2-5a62ff5b2c9c",
   "metadata": {},
   "source": [
    "## Student Was Early Headstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef1b7-0e2a-4ab1-a3b4-1771c58c7509",
   "metadata": {},
   "source": [
    "## Student Was Head Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9ee9b52-b2c3-4f9e-a8ba-f7016b8e98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once everything is done: drop unnecesary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece5bc2-d63b-41d6-97b7-d63da2e8f1db",
   "metadata": {},
   "source": [
    "# Next Section: Likert Scale Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6279c-22f0-417d-b643-47474cefcc2e",
   "metadata": {},
   "source": [
    "# Next Section: Open Interview Questions\n",
    "\n",
    "perhaps we can use data analysis to see how sentiments change\n",
    "https://www.surveypractice.org/article/25699-what-to-do-with-all-those-open-ended-responses-data-visualization-techniques-for-survey-researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad60cd04-403d-4d24-9b7b-033e87b382fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:63\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over the columns to translate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_translate:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Translate the non-null values in the column\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     df_sample[column] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over the columns to translate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_translate:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Translate the non-null values in the column\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     df_sample[column] \u001b[38;5;241m=\u001b[39m df_sample[column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(x) \u001b[38;5;28;01melse\u001b[39;00m x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:210\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    209\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m--> 210\u001b[0m data, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:102\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    100\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxxxx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#dummy default value here as it is not used by api client\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebapp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type, query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m    105\u001b[0m                             token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m    107\u001b[0m url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:199\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:67\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find TKK token for this request.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee https://github.com/ssut/py-googletrans/issues/234 for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details."
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']\n",
    "\n",
    "# Create an instance of the Translator class\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# Iterate over the columns to translate\n",
    "for column in columns_to_translate:\n",
    "    # Translate the non-null values in the column\n",
    "    df_sample[column] = df_sample[column].apply(lambda x: translator.translate(x, dest='en').text if pd.notnull(x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8da15bc-743b-4ac1-b0a3-71c64338d5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:63\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df[column]):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(value):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Detect if the value is in Spanish\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;66;03m# Translate the value from Spanish to English\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             translation \u001b[38;5;241m=\u001b[39m translator\u001b[38;5;241m.\u001b[39mtranslate(value, src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m, dest\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;66;03m# Update the translated value in the DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:285\u001b[0m, in \u001b[0;36mTranslator.detect\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(lang)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 285\u001b[0m data, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# actual source language that will be recognized by Google Translator when the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# src passed is equal to auto.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:102\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    100\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxxxx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#dummy default value here as it is not used by api client\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebapp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type, query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m    105\u001b[0m                             token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m    107\u001b[0m url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:199\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:67\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find TKK token for this request.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee https://github.com/ssut/py-googletrans/issues/234 for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details."
     ]
    }
   ],
   "source": [
    "#first need to translate spanish-language to english\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']\n",
    "\n",
    "# Create a table to display translation information\n",
    "table = Table(title=\"Translation Information\")\n",
    "table.add_column(\"Variable\", justify=\"left\", style=\"cyan\")\n",
    "table.add_column(\"Rows Translated\", justify=\"right\", style=\"green\")\n",
    "table.add_column(\"% Translated (Non-Missing)\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "# Iterate over the columns to translate\n",
    "for column in columns_to_translate:\n",
    "    translated_count = 0\n",
    "    non_missing_count = df[column].notnull().sum()\n",
    "    translator = Translator(service_urls=['translate.google.com'])  # Create an instance of the Translator class\n",
    "    for i, value in enumerate(df[column]):\n",
    "        if pd.notnull(value):\n",
    "            # Detect if the value is in Spanish\n",
    "            if translator.detect(value).lang == \"es\":\n",
    "                # Translate the value from Spanish to English\n",
    "                translation = translator.translate(value, src='es', dest='en')\n",
    "                # Update the translated value in the DataFrame\n",
    "                df.loc[i, column] = translation.text\n",
    "                translated_count += 1\n",
    "    # Calculate the percentage of translated non-missing rows\n",
    "    percent_translated = translated_count / non_missing_count * 100 if non_missing_count > 0 else 0\n",
    "    # Add a row to the table\n",
    "    table.add_row(column, str(translated_count), f\"{percent_translated:.2f}%\")\n",
    "\n",
    "# Display the table\n",
    "console = Console()\n",
    "console.print(table)\n",
    "\n",
    "\n",
    "\n",
    "#scrub names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39607fda-6bc8-4493-a872-cd304150ac36",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Columns\n",
    "\n",
    "After finishing data cleaning, remove excess columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f5a74-e6d5-4ad7-adc1-cbfe26822685",
   "metadata": {},
   "outputs": [],
   "source": [
    "##REMOVE UNNECESSARY COLUMNS\n",
    "# guardian_vendor_id, interview_id, interviewer_id, interviewer,\n",
    "#interviewer_vendor_id, student_staff_vendor_id, student_vendor_id\n",
    "#guardian, guardian_employment, guardian_highest_education\n",
    "\n",
    "# List of columns to be removed\n",
    "columns_to_remove = ['created_at', 'guardian', 'guardian_employment', 'guardian_highest_education', 'guardian_birth_date'\n",
    "                     'guardian_vendor_id', 'interview_id', 'interviewer_id', 'mode',\n",
    "                     'interviewer', 'interviewer_vendor_id', 'student', 'student_staff_vendor_id', 'student_birth_date'\n",
    "                     'student_vendor_id', 'student_staff', 'student_staff_id']\n",
    "\n",
    "# Removing the columns from the DataFrame\n",
    "df_sample = df_sample.drop(columns=columns_to_remove)\n",
    "\n",
    "#save updates to working csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
