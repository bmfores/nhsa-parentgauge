{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f266bcaa",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"images/nhsa_logo.png\" alt=\"Image\" style=\"text-align: left;\">\n",
    "</div>\n",
    "\n",
    "# Parent Gauge Data Analysis Project\n",
    "---\n",
    "## Data Wrangling Script and Documentation\n",
    "\n",
    "In this script, we will provide a step-by-step demonstration of how script is being cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ada7c4d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "#uses old version of google trans: pip3 install googletrans==3.1.0a0\n",
    "from googletrans import Translator\n",
    "\n",
    "#used for filling in missing genders --DISCLAIMER: I recognize the sensitivities of this matter and understand that this may not be totally accurate\n",
    "import gender_guesser.detector as gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880529ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_20998/428991487.py:9: DtypeWarning: Columns (13,18,20,30,38,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/intv_data.csv')\n"
     ]
    }
   ],
   "source": [
    "#Load the Data into the dataframe\n",
    "df = pd.read_excel('../data/INTVDATA.xlsx', sheet_name ='Main', engine ='openpyxl')\n",
    "######MAKE SURE TO SPECIFY DATATYPE LATER ON######\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)\n",
    "\n",
    "#read the new .csv file\n",
    "df = pd.read_csv('../data/intv_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b43f5",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb847af-89c4-4385-b576-a29fd0615f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Drop Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b05f7da0-19b2-4c92-bfef-9e340735cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 203\n"
     ]
    }
   ],
   "source": [
    "####DROP DUPLICATE ROWS####\n",
    "# Count the number of rows before dropping duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Drop duplicate rows based on all columns except the first two\n",
    "result = df.drop_duplicates(subset=df.columns[2:])\n",
    "\n",
    "# Count the number of rows after dropping duplicates\n",
    "rows_after = len(result)\n",
    "\n",
    "# Calculate and print the number of rows dropped\n",
    "rows_dropped = rows_before - rows_after\n",
    "print(f\"Number of duplicate rows dropped: {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aefeac-e571-4881-8a0a-ab1a8b21ad62",
   "metadata": {},
   "source": [
    "## Sample Generation\n",
    "\n",
    "Note: You can run this code again if you would like to reset the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdede69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a sample of 10% of the total dataset\n"
     ]
    }
   ],
   "source": [
    "#Because the main dataset is too large for data cleaning, \n",
    "#construct a small sample for faster processing. Once I am done coding, we will use the entire dataset.\n",
    "df_sample = df.sample(frac=0.1)\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n",
    "print(\"Created a sample of 10% of the total dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc37ee6",
   "metadata": {},
   "source": [
    "## Main Data Cleaning\n",
    "\n",
    "This is a summary of all the data cleaning and reformatting steps that were conducted.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county.\n",
    "- **Program** - I identified the corresponding state and county."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067236d-26f6-4dcf-98b9-e9b68751588b",
   "metadata": {},
   "source": [
    "# Code to Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77e40-2bab-49b2-b109-695cd4b10243",
   "metadata": {},
   "source": [
    "## Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f9bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##code to clean\n",
    "##PROGRAM\n",
    "unique_programs = df_sample['program'].unique().tolist()\n",
    "\n",
    "# Sort the list in place\n",
    "unique_programs.sort()\n",
    "\n",
    "#create a text file of the unique programs\n",
    "with open('../data/unique_programs.txt', 'w') as f:\n",
    "    for item in unique_programs:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9231c-5aba-4b41-b54f-5b91242b2d42",
   "metadata": {},
   "source": [
    "## Created_at\n",
    " drop this feature, as it is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f59120a-542e-40b9-90f1-75f2308c0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.drop('created_at', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04603ece-23cd-4ff9-aa5d-daee20bde4de",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e79c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATE##\n",
    "# Iterate over the entries in the 'date' column\n",
    "for i, date in enumerate(df_sample['date']):\n",
    "    try:\n",
    "        # Try to convert the date to datetime format\n",
    "        pd.to_datetime(date, format='mixed')\n",
    "    except Exception:\n",
    "        print(f\"An error occurred at index {i} with the date: {date}\")\n",
    "        \n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], errors='coerce')\n",
    "\n",
    "#Create separate 'year', 'month', and 'day' columns\n",
    "df_sample['date_year'] = df_sample['date'].dt.year\n",
    "df_sample['date_month'] = df_sample['date'].dt.month\n",
    "df_sample['date_day'] = df_sample['date'].dt.day\n",
    "\n",
    "#save to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3600a-9e0d-45bb-8b0d-dabedd5d447b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dummy variables have been created, breaking the three categorical variables into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb51bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluation##\n",
    "#use one-hot encoding to create dummy variables in preparation for regression.\n",
    "#NOTE: This, however eliminates the original 'evaluation' column\n",
    "df_sample = pd.get_dummies(df_sample, columns=['evaluation'])\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n",
    "#**pending: drop excess 'evaluation_evaluation' column**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dea179-7f4f-4d83-8aea-1a15e8aabb91",
   "metadata": {},
   "source": [
    "## Evaluation Year\n",
    "\n",
    "The original format of the evaluation year was formatted as 2016-2017. For easier analysis, the start and end year have been split up into two columns, \"evaluation_start_year\" and \"evaluation_end_year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70434815",
   "metadata": {},
   "outputs": [],
   "source": [
    "##evaluation_year##\n",
    "# Split the 'evaluation_year' column into two separate columns 'start_year' and 'end_year'\n",
    "df_sample[['evaluation_start_year', 'evaluation_end_year']] = df_sample['evaluation_year'].str.split('-', expand=True)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91907a-6497-4597-b2fa-51833440742e",
   "metadata": {},
   "source": [
    "## Guardian Name\n",
    "\n",
    "the name should be scrubbed for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3f887bb-2262-4dce-b97f-51dcf103d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('guardian', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3562b-8532-4949-9da0-715f7d47dd6e",
   "metadata": {},
   "source": [
    "## Guardian Employment\n",
    "\n",
    "upon further inspection, it seems that the guardian employment is tied to the guardian_id, and it is not possible to further fill in missing values. note that over 60% of guardian_employment is missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ec11-d8cb-4c4b-8708-8ad9f2afa55e",
   "metadata": {},
   "source": [
    "## Guardian Enrollment Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60356e33-2ece-47e9-9fec-f3b3621ccc1a",
   "metadata": {},
   "source": [
    "## Guardian Highest Education\n",
    "\n",
    "same as guardian_employment, no referencing of the guardian_id will fill in missing blanks. Note that over 60% of guardian employment is misssing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91308fcc-6658-46b1-8b79-e0d99dea5e2d",
   "metadata": {},
   "source": [
    "## Guardian, Hispanic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cf8fc10-fd5d-4859-84e6-ea606898c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"student_hispanic\", \n",
    "#\"student_native_language\", \"language of interview\", \n",
    "# Define a function to fill missing values in guardian_hispanic column based on conditions\n",
    "def fill_guardian_hispanic(row):\n",
    "    if pd.isnull(row['guardian_hispanic']) or row['guardian_hispanic'] not in ['Yes', 'No']:\n",
    "        if row['student_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['student_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    else:\n",
    "        return row['guardian_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in guardian_hispanic column\n",
    "df_sample['guardian_hispanic'] = df_sample.apply(fill_guardian_hispanic, axis=1)\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "df_sample['guardian_hispanic'] = df_sample['guardian_hispanic'].map({'Yes': True, 'No': False})\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6765367-b42f-4132-a26e-d852345480de",
   "metadata": {},
   "source": [
    "## Guardian Native Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a92fbe-0191-4258-a1a7-5d4fa4b0e2f8",
   "metadata": {},
   "source": [
    "## Guardian Race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201ede-9c7f-4763-9d8c-d792f76f4a1e",
   "metadata": {},
   "source": [
    "## Guardian Date of Birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48824aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##guardian_birth_date##\n",
    "#– use DOB and interview year to determine guardian’s age during time of interview. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b7e9-9333-4e1c-b5e2-7a5285052490",
   "metadata": {},
   "source": [
    "## Guardian Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9680dbf4-ac28-4897-a3e2-174f71d51789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use gender_guesser library to fill in missing values\n",
    "def guess_gender(name):\n",
    "    d = gender.Detector()\n",
    "    gender_guess = d.get_gender(name)\n",
    "    return gender_guess\n",
    "\n",
    "# Apply guess_gender function to fill missing values in 'guardian_sex' column\n",
    "df_sample['guardian_sex'] = df_sample.apply(lambda row: guess_gender(row['guardian']) if pd.isnull(row['guardian_sex']) else row['guardian_sex'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69af102a-e7dd-4afa-9a29-b3ba193604d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###use this section to clean up inconsistencies#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using direct mapping to create dummy variable out of guardian_sex\n",
    "df_sample['female'] = (df_sample['guardian_sex'] == 'Female').astype(int)\n",
    "\n",
    "#save updates to csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c051d4-09b8-4a7f-ac1e-99568cf937c0",
   "metadata": {},
   "source": [
    "## Guardian Vendor ID\n",
    "\n",
    "consider dropping this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c09d09c1-d191-4185-ab7c-07110e88843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['guardian_vendor_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##need to scrub the name, drop the column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian_vendor_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6696\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6697\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['guardian_vendor_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "##need to scrub the name, drop the column\n",
    "df_sample = df_sample.drop('guardian_vendor_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ff1a3-2d2b-45fe-a196-43733ee1493b",
   "metadata": {},
   "source": [
    "## Interview ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db1f4e82-c866-4129-ae85-3498c2fd88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "df_sample = df_sample.drop('interview_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45a88d-0e72-49c2-93db-ceb3e40e2f43",
   "metadata": {},
   "source": [
    "## Interviewer Name\n",
    "\n",
    "Drop for privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b08e297-5f3b-4f4c-b97a-f309a7b00efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "df_sample = df_sample.drop('interviewer', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e7015-af4b-4f45-839a-aaa8126e0974",
   "metadata": {},
   "source": [
    "## Interviewer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3463d3e0-b0f5-4c06-9c5f-5a5851c496c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "df_sample = df_sample.drop('interviewer_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571618f-6aa2-49a7-b219-4598690a8a2d",
   "metadata": {},
   "source": [
    "## Interviewer Vendor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ea1c303-cc4e-48ac-8e5e-eb6678a1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the column\n",
    "df_sample = df_sample.drop('interviewer_vendor_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bb666-c052-4c9a-a373-7a2638ddf708",
   "metadata": {},
   "source": [
    "## Language of Interview\n",
    "\n",
    "Here, (1) remove invalid languages i.e. \"language\". (2) next, use the guardian ID to fill in missing values. (3) Lastly consider removing or imputing rows that have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e633270e-e783-42de-9405-347f903ed843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: remove invalid languages i.e. language\n",
    "\n",
    "#step 2: use guardian_id to fill in missing values\n",
    "\n",
    "#step 3: remove or impute rows that have missing values\n",
    "#do this by checking if any of the open interview responses were in a certain language, if they are, apply corresponding language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820c9b2-5124-4242-9cfd-7c0c568ff999",
   "metadata": {},
   "source": [
    "## Mode of Interview\n",
    "\n",
    "convert to dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c772ac3-3273-4e95-94a5-f2fafb0efde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode\n",
      "In-person    120277\n",
      "Phone         84356\n",
      "mode            204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts = df['mode'].value_counts()\n",
    "\n",
    "print(value_counts)\n",
    "\n",
    "#impute missing values proportionally\n",
    "\n",
    "#convert to dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca48e3-011c-40fa-80af-e53e44b0d3b0",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d4b94-f760-4f48-8314-2e352a8a0ee9",
   "metadata": {},
   "source": [
    "## Student Name\n",
    "\n",
    "drop the name for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b6f7849-53a0-4d72-80fc-67e143d98eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d460a-4967-4f05-ae00-2384c66eb7ed",
   "metadata": {},
   "source": [
    "## Student Enrollment Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db8661-9234-4a9e-8585-81f63f07fd08",
   "metadata": {},
   "source": [
    "## Student Disability Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723b93ec-c71e-40f9-b34a-a980b54da879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing rows in 'student_hispanic' column: 1136\n"
     ]
    }
   ],
   "source": [
    "# Count the number of missing rows in 'guardian_hispanic' column\n",
    "missing_count = df_sample['student_hispanic'].isnull().sum()\n",
    "\n",
    "# Print the number of missing rows\n",
    "print(\"Number of missing rows in 'student_hispanic' column:\", missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6f308-0d54-4e57-a7df-9144cee7c5dd",
   "metadata": {},
   "source": [
    "## Student, Hispanic\n",
    "##remove anything that is not \"yes\" or \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd96b756-0128-4ce0-976d-9ed748f4de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "# Update the guardian_hispanic column\n",
    "df_sample.loc[~df_sample[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"guardian_hispanic\", \"student_native_language\"\n",
    "# Define a function to fill missing values in student_hispanic column based on conditions\n",
    "def fill_student_hispanic(row):\n",
    "    if pd.isnull(row['student_hispanic']):\n",
    "        if row['guardian_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['guardian_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif pd.notnull(row['student_native_language']):\n",
    "            return 'No'\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row['student_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in student_hispanic column\n",
    "df_sample['student_hispanic'] = df_sample.apply(fill_student_hispanic, axis=1)\n",
    "\n",
    "##---------------------------##\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "#df_sample = pd.get_dummies(df_sample, columns=[\"guardian_hispanic\"], prefix=\"guardian_hispanic\", drop_first=True)\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b218699-95ec-4e6b-a7ae-74b31d48bb5e",
   "metadata": {},
   "source": [
    "## Student, ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d3621-4b56-45a5-8328-971053cb8350",
   "metadata": {},
   "source": [
    "## Student Birth Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b75e0-4753-4a15-8d2d-068d65477886",
   "metadata": {},
   "source": [
    "## Student in last year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5fbc1-5ea4-4de7-ad32-6a3c9b995b47",
   "metadata": {},
   "source": [
    "## Student Native Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a46cb-a2b7-4d1b-91b4-c6f5ca63f617",
   "metadata": {},
   "source": [
    "## Student Program Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19ee73-5275-450c-bddd-98476604d5dc",
   "metadata": {},
   "source": [
    "## Student Race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb8856-0702-4e8c-9eed-aba58ec89b13",
   "metadata": {},
   "source": [
    "## Student Service Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a8e69-c4e4-4d25-8c87-cc9ce23bd330",
   "metadata": {},
   "source": [
    "## Student Sex\n",
    "\n",
    "use gender_guesser again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "268dbcb7-8e4a-43ec-9a1d-03b44d6f5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_gender(name):\n",
    "    d = gender.Detector()\n",
    "    gender_guess = d.get_gender(name)\n",
    "    return gender_guess\n",
    "\n",
    "# Apply guess_gender function to fill missing values in 'student_sex' column\n",
    "df_sample['student_sex'] = df_sample.apply(lambda row: guess_gender(row['student']) if pd.isnull(row['student_sex']) else row['student_sex'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fc8f9-47e2-4a3f-9ca2-24d4a9609aa1",
   "metadata": {},
   "source": [
    "## Student Staff\n",
    "\n",
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bf054-62bb-4663-8f34-ee97346b285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student_staff', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82383d2-47ab-4d3f-80e5-93fbdc3153af",
   "metadata": {},
   "source": [
    "## Student Staff ID\n",
    "\n",
    "drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "948ca9df-dd9f-4e5f-89be-7cc5f9c8e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student_staff_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341642f-841e-4452-ba8a-2c893685dfe3",
   "metadata": {},
   "source": [
    "## Student Staff Vendor ID\n",
    "\n",
    "consider removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d69d6bb-dd74-43cd-bb55-0012273af294",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student_staff_vendor_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0c85e-4433-49e5-9525-61515f73a7e0",
   "metadata": {},
   "source": [
    "## Student Vendor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7cfb50c1-9513-4a98-8419-f71f85507547",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to scrub the name, drop the column\n",
    "#df_sample = df_sample.drop('student_staff_vendor_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82471932-cc4a-45ec-8ee2-5a62ff5b2c9c",
   "metadata": {},
   "source": [
    "## Student Was Early Headstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef1b7-0e2a-4ab1-a3b4-1771c58c7509",
   "metadata": {},
   "source": [
    "## Student Was Head Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9ee9b52-b2c3-4f9e-a8ba-f7016b8e98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once everything is done: drop unnecesary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece5bc2-d63b-41d6-97b7-d63da2e8f1db",
   "metadata": {},
   "source": [
    "# Next Section: Likert Scale Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6279c-22f0-417d-b643-47474cefcc2e",
   "metadata": {},
   "source": [
    "# Next Section: Open Interview Questions\n",
    "\n",
    "perhaps we can use data analysis to see how sentiments change\n",
    "https://www.surveypractice.org/article/25699-what-to-do-with-all-those-open-ended-responses-data-visualization-techniques-for-survey-researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad60cd04-403d-4d24-9b7b-033e87b382fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:63\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over the columns to translate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_translate:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Translate the non-null values in the column\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     df_sample[column] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate over the columns to translate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_translate:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Translate the non-null values in the column\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     df_sample[column] \u001b[38;5;241m=\u001b[39m df_sample[column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(x) \u001b[38;5;28;01melse\u001b[39;00m x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:210\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    209\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m--> 210\u001b[0m data, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:102\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    100\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxxxx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#dummy default value here as it is not used by api client\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebapp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type, query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m    105\u001b[0m                             token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m    107\u001b[0m url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:199\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:67\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find TKK token for this request.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee https://github.com/ssut/py-googletrans/issues/234 for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details."
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']\n",
    "\n",
    "# Create an instance of the Translator class\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "\n",
    "# Iterate over the columns to translate\n",
    "for column in columns_to_translate:\n",
    "    # Translate the non-null values in the column\n",
    "    df_sample[column] = df_sample[column].apply(lambda x: translator.translate(x, dest='en').text if pd.notnull(x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8da15bc-743b-4ac1-b0a3-71c64338d5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:63\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df[column]):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(value):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Detect if the value is in Spanish\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;66;03m# Translate the value from Spanish to English\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             translation \u001b[38;5;241m=\u001b[39m translator\u001b[38;5;241m.\u001b[39mtranslate(value, src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m, dest\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;66;03m# Update the translated value in the DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:285\u001b[0m, in \u001b[0;36mTranslator.detect\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(lang)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 285\u001b[0m data, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# actual source language that will be recognized by Google Translator when the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# src passed is equal to auto.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/client.py:102\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    100\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxxxx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#dummy default value here as it is not used by api client\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebapp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_type, query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[1;32m    105\u001b[0m                             token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[1;32m    107\u001b[0m url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:199\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googletrans/gtoken.py:67\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find TKK token for this request.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee https://github.com/ssut/py-googletrans/issues/234 for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details."
     ]
    }
   ],
   "source": [
    "#first need to translate spanish-language to english\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']\n",
    "\n",
    "# Create a table to display translation information\n",
    "table = Table(title=\"Translation Information\")\n",
    "table.add_column(\"Variable\", justify=\"left\", style=\"cyan\")\n",
    "table.add_column(\"Rows Translated\", justify=\"right\", style=\"green\")\n",
    "table.add_column(\"% Translated (Non-Missing)\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "# Iterate over the columns to translate\n",
    "for column in columns_to_translate:\n",
    "    translated_count = 0\n",
    "    non_missing_count = df[column].notnull().sum()\n",
    "    translator = Translator(service_urls=['translate.google.com'])  # Create an instance of the Translator class\n",
    "    for i, value in enumerate(df[column]):\n",
    "        if pd.notnull(value):\n",
    "            # Detect if the value is in Spanish\n",
    "            if translator.detect(value).lang == \"es\":\n",
    "                # Translate the value from Spanish to English\n",
    "                translation = translator.translate(value, src='es', dest='en')\n",
    "                # Update the translated value in the DataFrame\n",
    "                df.loc[i, column] = translation.text\n",
    "                translated_count += 1\n",
    "    # Calculate the percentage of translated non-missing rows\n",
    "    percent_translated = translated_count / non_missing_count * 100 if non_missing_count > 0 else 0\n",
    "    # Add a row to the table\n",
    "    table.add_row(column, str(translated_count), f\"{percent_translated:.2f}%\")\n",
    "\n",
    "# Display the table\n",
    "console = Console()\n",
    "console.print(table)\n",
    "\n",
    "\n",
    "\n",
    "#scrub names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39607fda-6bc8-4493-a872-cd304150ac36",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f5a74-e6d5-4ad7-adc1-cbfe26822685",
   "metadata": {},
   "outputs": [],
   "source": [
    "##REMOVE UNNECESSARY COLUMNS\n",
    "# guardian_vendor_id, interview_id, interviewer_id, interviewer,\n",
    "#interviewer_vendor_id, student_staff_vendor_id, student_vendor_id\n",
    "\n",
    "# List of columns to be removed\n",
    "columns_to_remove = ['guardian_vendor_id', 'interview_id', 'interviewer_id', \n",
    "                     'interviewer', 'interviewer_vendor_id', 'student_staff_vendor_id', \n",
    "                     'student_vendor_id']\n",
    "\n",
    "# Removing the columns from the DataFrame\n",
    "df_sample = df_sample.drop(columns=columns_to_remove)\n",
    "\n",
    "#save updates to working csv\n",
    "df_sample.to_csv('../data/sample_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
