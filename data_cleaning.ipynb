{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f266bcaa",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"images/nhsa_logo.png\" alt=\"Image\" style=\"text-align: left;\">\n",
    "</div>\n",
    "\n",
    "# Parent Gauge Data Analysis Project\n",
    "---\n",
    "## Data Wrangling Script and Documentation\n",
    "\n",
    "In this script, we will provide a step-by-step demonstration of how script is being cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada7c4d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from IPython.display import display\n",
    "\n",
    "# uses old version of google trans: pip3 install googletrans==3.1.0a0\n",
    "from googletrans import Translator\n",
    "\n",
    "# Utility function for gender imputation - DISCLAIMER: Sensitivity and Accuracy Considerations\n",
    "# This function is intended for filling in missing gender values for statistical purposes only.\n",
    "# Please note that gender imputation methods may not accurately reflect an individual's gender identity.\n",
    "# Use caution and sensitivity when interpreting or applying these imputed values.\n",
    "import gender_guesser.detector as gender\n",
    "from genderize import Genderize\n",
    "\n",
    "#utilize a Named Entity Recognition (NER) library to detect and remove named entities like names\n",
    "#import spacy\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16e3169b-daad-48f6-81b5-7b2f4e85e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data into the dataframe\n",
    "df = pd.read_excel('../data/INTVDATA.xlsx', sheet_name ='Main', engine ='openpyxl')\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880529ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_59052/4054036919.py:2: DtypeWarning: Columns (13,18,20,30,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/intv_data.csv')\n"
     ]
    }
   ],
   "source": [
    "#read the new .csv file\n",
    "df = pd.read_csv('../data/intv_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b43f5",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb847af-89c4-4385-b576-a29fd0615f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Drop Duplicate Rows\n",
    "\n",
    "A majority of the duplicate rows in the parent gauge dataset involve rows that are duplicates of the header row. We will use a random variable such as 'date' and remove rows that equal the name of the variable. Afterwards, we will drop additional duplicates, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b05f7da0-19b2-4c92-bfef-9e340735cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows dropped: 204\n"
     ]
    }
   ],
   "source": [
    "# To make sure all duplicate \"header\" rows are eliminated, we pick a random column,\n",
    "# remove rows where 'date' column is equal to 'date', except the first row\n",
    "# Initialize a counter for deleted rows\n",
    "deleted_date_rows = 0\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    if row['date'] == 'date':\n",
    "        # Remove the row if the date matches\n",
    "        df.drop(i, inplace=True)\n",
    "        deleted_date_rows += 1\n",
    "\n",
    "# Count the number of rows before dropping duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Drop duplicate rows based on all columns except the first two, since they are indexed\n",
    "result = df.drop_duplicates(subset=df.columns[2:])\n",
    "        \n",
    "# Count the number of rows after dropping duplicates\n",
    "rows_after = len(result)\n",
    "\n",
    "# Calculate and print the number of rows dropped\n",
    "rows_dropped = (rows_before - rows_after) + deleted_date_rows\n",
    "print(f\"Number of duplicate rows dropped: {rows_dropped}\")\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aefeac-e571-4881-8a0a-ab1a8b21ad62",
   "metadata": {},
   "source": [
    "## Sample Data Frame Generation\n",
    "\n",
    "Note: You can run this code again if you would like to reset the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdede69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a sample of 10% of the total dataset\n"
     ]
    }
   ],
   "source": [
    "#Because the main dataset is too large for data cleaning, \n",
    "#construct a small sample for faster processing. Once I am done coding, we will use the entire dataset.\n",
    "#df_sample = df.sample(frac=0.1)\n",
    "#df_sample.to_csv('../data/sample_data.csv', index=False)\n",
    "\n",
    "#print(\"Created a sample of 10% of the total dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc37ee6",
   "metadata": {},
   "source": [
    "## Main Data Cleaning\n",
    "\n",
    "This is a summary of all the data cleaning and reformatting steps that were conducted.\n",
    "- **Program** - I identified the corresponding state and county."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067236d-26f6-4dcf-98b9-e9b68751588b",
   "metadata": {},
   "source": [
    "# Code to Clean\n",
    "\n",
    "This section examines each variable and transforms and cleans them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77e40-2bab-49b2-b109-695cd4b10243",
   "metadata": {},
   "source": [
    "## Center\n",
    "\n",
    "For the center, we want to correspond each center to their respective state and geographic location for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36f9bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_centers = df['center'].unique().tolist()\n",
    "\n",
    "# Sort the list in place\n",
    "unique_centers.sort()\n",
    "\n",
    "#create a text file of the unique programs\n",
    "with open('../data/unique_centers.txt', 'w') as f:\n",
    "    for item in unique_centers:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04603ece-23cd-4ff9-aa5d-daee20bde4de",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "Here, we will break down the date into three columns, seperating the year, month, and day. This code performs data cleaning on a column called 'date' in the dataset. It first checks each entry in the column for any errors in the date format, printing an error message if any are found. Then, it converts the 'date' column to a standard datetime format and creates new columns for the year, month, and day extracted from the dates. Finally, the cleaned data is saved to a CSV file. This code ensures that the dates are properly formatted, allows for easy analysis based on different time periods, and provides a clean dataset for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d038ea4f-35ab-4536-b01a-dc45116dd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX ERRONEOUS DATES\n",
    "# TIP: There are less than 100 erroneous rows, 96%, or most of the, 'date' entries match the same year as 'created_at'\n",
    "# If the 'date' year is deemed to be erroneous, replace the 'date' year with the 'created_at' year instead\n",
    "\n",
    "# Convert 'created_at' and 'date' columns to datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Determine current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Define a function to replace the year of a given date with the year of 'created_at':\n",
    "def replace_year(row):\n",
    "    # Extract the years\n",
    "    date_year = row['date'].year\n",
    "    created_at_year = row['created_at'].year\n",
    "    \n",
    "    # Check if the year in 'date' is below 2016 or the current year\n",
    "    if date_year < 2016 or date_year > current_year:\n",
    "        # Replace the year part of 'date' with 'created_at' year\n",
    "        return row['date'].replace(year=created_at_year)\n",
    "    else:\n",
    "        return row['date']\n",
    "\n",
    "# NOTE: Since Parent Gauge was officially implemented in 2017, anything before 2016 and past the current year should probablybe considered erroneous\n",
    "# Identify erroneous dates where the year is below 2016 or above the current year\n",
    "# Apply custom function to DataFrame\n",
    "df['date'] = df.apply(replace_year, axis=1)\n",
    "\n",
    "#Copy existing dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e79c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATE INTO MONTH, DAY, AND YEAR\n",
    "# Iterate over the entries in the 'date' column\n",
    "for i, date in enumerate(df['date']):\n",
    "    try:\n",
    "        # Try to convert the date to datetime format\n",
    "        pd.to_datetime(date, format='mixed')\n",
    "    except Exception:\n",
    "        print(f\"An error occurred at index {i} with the date: {date}\")\n",
    "        \n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Create separate 'year', 'month', and 'day' columns\n",
    "df['date_year'] = df['date'].dt.year\n",
    "df['date_month'] = df['date'].dt.month\n",
    "df['date_day'] = df['date'].dt.day\n",
    "\n",
    "# save to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dea179-7f4f-4d83-8aea-1a15e8aabb91",
   "metadata": {},
   "source": [
    "## Evaluation Year\n",
    "\n",
    "The original format of the evaluation year was formatted as '2016-2017', for instance. For easier analysis, the start and end year have been split up into two columns, \"evaluation_start_year\" and \"evaluation_end_year.\" Moreover, we convert the new variables into a faster data type. Lastly, try to fix any erroneous years, then remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "70434815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Split the 'evaluation_year' column into two separate columns 'start_year' and 'end_year'\n",
    "df[['evaluation_start_year', 'evaluation_end_year']] = df['evaluation_year'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 'evaluation_start_year' and 'evaluation_end_year to int16, handling invalid values\n",
    "df['evaluation_start_year'] = pd.to_numeric(df['evaluation_start_year'], errors='coerce').astype('Int16')\n",
    "df['evaluation_end_year'] = pd.to_numeric(df['evaluation_end_year'], errors='coerce').astype('Int16')\n",
    "\n",
    "# Create intermediate 'student_enrollment_date_' column to datetime, in preparation for referencing\n",
    "df['student_enrollment_date_temp'] = pd.to_datetime(df['student_enrollment_date'], format='mixed', errors='coerce')\n",
    "\n",
    "# NOTE: Parent guage officially started in 2017, so years far from that range are erroneous.\n",
    "# FIX errneous rows that were supposed to be a valid year\n",
    "# Store the indices of rows with updated evaluation_start_year\n",
    "updated_indices = df[(df['evaluation_start_year'] < 2016) | (df['evaluation_start_year'] > current_year)].index\n",
    "\n",
    "# Update evaluation_start_year based on evaluation and student_enrollment_date, which is less erroneous\n",
    "df.loc[df['evaluation_start_year'] < 2016, 'evaluation_start_year'] = df.loc[df['evaluation_start_year'] < 2016].apply(\n",
    "    lambda row: row['student_enrollment_date_temp'].year if row['evaluation'] == 'Initial' else row['student_enrollment_date_temp'].year + 1, axis=1\n",
    ")\n",
    "\n",
    "df.loc[df['evaluation_start_year'] > current_year, 'evaluation_start_year'] = df.loc[df['evaluation_start_year'] > current_year].apply(\n",
    "    lambda row: row['student_enrollment_date_temp'].year if row['evaluation'] == 'Initial' else row['student_enrollment_date_temp'].year + 1, axis=1\n",
    ")\n",
    "\n",
    "# Update evaluation_end_year only for the updated rows\n",
    "df.loc[updated_indices, 'evaluation_end_year'] = df.loc[updated_indices, 'evaluation_start_year'] + 1\n",
    "\n",
    "# Clear erroneous values where 'evaluation_start_year' is below 2016 or above current_year\n",
    "df.loc[df['evaluation_start_year'] < 2016, ['evaluation_year', 'evaluation_start_year', 'evaluation_end_year']] = None\n",
    "df.loc[df['evaluation_start_year'] > current_year, ['evaluation_year', 'evaluation_start_year', 'evaluation_end_year']] = None\n",
    "\n",
    "# drop intermediate variable\n",
    "df = df.drop('student_enrollment_date_temp', axis=1)\n",
    "\n",
    "#copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3600a-9e0d-45bb-8b0d-dabedd5d447b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dummy variables have been created, breaking the three categorical variables into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb51bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one-hot encoding to create dummy variables in preparation for regression.\n",
    "#WARNING: This, however eliminates the original 'evaluation' column\n",
    "df = pd.get_dummies(df, columns=['evaluation'])\n",
    "\n",
    "#drop excess 'evaluation_evaluation' column, if it exists.\n",
    "if 'evaluation_evaluation' in df.columns:\n",
    "    df.drop('evaluation_evaluation', axis=1, inplace=True)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ec11-d8cb-4c4b-8708-8ad9f2afa55e",
   "metadata": {},
   "source": [
    "## Guardian Enrollment Date\n",
    "\n",
    "Use this variable to determine how many years the parent has been in the parent gauge program.\n",
    "\n",
    "problem: 70% of guardian IDs have no enrollment date, but this is an important part of the analysis. need to retroactively determine what the enrollment date is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91308fcc-6658-46b1-8b79-e0d99dea5e2d",
   "metadata": {},
   "source": [
    "## Guardian, Hispanic\n",
    "\n",
    "There are missing values, but many other variables including the guardian's native language, student's native language, whether the student is hispanic, and what language the interview was used—to determine whether the guardian was hispanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cf8fc10-fd5d-4859-84e6-ea606898c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\"\n",
    "#reconcile \"dont know\" and \"refused\" values\n",
    "####pending...\n",
    "\n",
    "# Update the guardian_hispanic column\n",
    "df.loc[~df[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"student_hispanic\", \n",
    "#\"student_native_language\", \"language of interview\", \n",
    "# Define a function to fill missing values in guardian_hispanic column based on conditions\n",
    "def fill_guardian_hispanic(row):\n",
    "    if pd.isnull(row['guardian_hispanic']) or row['guardian_hispanic'] not in ['Yes', 'No']:\n",
    "        if row['student_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['student_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    else:\n",
    "        return row['guardian_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in guardian_hispanic column\n",
    "df['guardian_hispanic'] = df.apply(fill_guardian_hispanic, axis=1)\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "df['guardian_hispanic'] = df['guardian_hispanic'].map({'Yes': True, 'No': False})\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6765367-b42f-4132-a26e-d852345480de",
   "metadata": {},
   "source": [
    "## Guardian Native Language\n",
    "\n",
    "7% of rows for guardian native language are missing, While the student's native language is not necessarily the same as the guardian's native language, we might be able to reference for statistical purposes. This will cut down missing to 2.5%. 91% are in English and almost 7% are in Spanish. Imputing the values with modal value (English) does not drastically change the numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1991b9b8-dabe-49c9-85ff-eee6f6b8bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "# Reference 'language' of interview', if interview was done in Spanish, assume guardian's native language is Spanish\n",
    "df.loc[df['guardian_native_language'].isnull() & (df['language'] == 'Spanish'), 'guardian_native_language'] = 'Spanish'\n",
    "\n",
    "# Then reference 'student_native_language' to fill in other missing values\n",
    "df['guardian_native_language'] = df['guardian_native_language'].fillna(df['student_native_language'])\n",
    "\n",
    "# Afterwards, reference 'guardian_id' as well\n",
    "df['guardian_native_language'] = df['guardian_native_language'].fillna(df.groupby('guardian_id')['guardian_native_language'].transform('first'))\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40844216-d915-4367-a2da-f601964423ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider grouping remaining languages into one category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a92fbe-0191-4258-a1a7-5d4fa4b0e2f8",
   "metadata": {},
   "source": [
    "## Guardian Race\n",
    "\n",
    "tasks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "585d7a0c-2040-4b73-a350-180c6f6d2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## clean and normalize\n",
    "# Convert to lowercase\n",
    "# Remove leading and trailing whitespace\n",
    "# Remove values that start with ', '\n",
    "df['guardian_race'] = df['guardian_race'].str.lower().str.strip().str.replace('^, ', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1ed74c6-7cac-45d1-a3e8-7352d7caa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simplify categories\n",
    "def simplify_race_category(category):\n",
    "    # Check for NaN (missing value)\n",
    "    if pd.isna(category):\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    # Convert category to lowercase for case-insensitive comparison\n",
    "    category = category.lower()\n",
    "    \n",
    "    # Count how many specific categories are in the value\n",
    "    count = 0\n",
    "    if \"white\" in category:\n",
    "        count += 1\n",
    "    if \"black\" in category or \"african american\" in category:\n",
    "        count += 1\n",
    "    if \"asian\" in category:\n",
    "        count += 1\n",
    "    if \"native\" in category or \"american indian\" in category or \"alaska native\" in category:\n",
    "        count += 1\n",
    "    if \"hawaiian\" in category or \"pacific islander\" in category:\n",
    "        count += 1\n",
    "    if \"other\" in category:\n",
    "        count += 1\n",
    "    if \"don't know\" in category or \"unknown\" in category:\n",
    "        count += 0\n",
    "    \n",
    "    # Based on count, return the simplified category\n",
    "    if count == 0:\n",
    "        return \"Unknown\"\n",
    "    elif count == 1:\n",
    "        if \"white\" in category:\n",
    "            return \"White\"\n",
    "        if \"black\" in category or \"african american\" in category:\n",
    "            return \"Black\"\n",
    "        if \"asian\" in category:\n",
    "            return \"Asian\"\n",
    "        if \"native\" in category or \"american indian\" in category or \"alaska native\" in category:\n",
    "            return \"Native American or Alaska Native\"\n",
    "        if \"hawaiian\" in category or \"pacific islander\" in category:\n",
    "            return \"Native Hawaiian or Pacific Islander\"\n",
    "        if \"other\" in category:\n",
    "            return \"Other\"\n",
    "    else:\n",
    "        return \"Multiracial\"\n",
    "\n",
    "# Apply the simplification function to the 'guardian_race' column\n",
    "df['guardian_race'] = df['guardian_race'].apply(simplify_race_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2023b15-31c7-4641-ae65-4f4522f86c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by guardian_id and fill missing values in guardian_race\n",
    "df['guardian_race'] = df.groupby('guardian_id')['guardian_race'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95525503-9396-43bb-a057-9601f833bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201ede-9c7f-4763-9d8c-d792f76f4a1e",
   "metadata": {},
   "source": [
    "## Guardian Date of Birth\n",
    "\n",
    "use guardian's DOB and interview year to determine guardian’s age during time of interview. We create a new variable named 'guardian_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48824aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use guardian's DOB and interview year to determine guardian’s age during time of interview. \n",
    "# CORRECT erroneous 'guardian_birth_date' and 'date' variable first before determining age\n",
    "\n",
    "# Step 1: Convert 'guardian_birth_date' and 'date' columns to datetime\n",
    "df['guardian_birth_date'] = pd.to_datetime(df['guardian_birth_date'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Step 2: try to map guardian_birth_date with corresponding guardian_id\n",
    "# Create a dictionary mapping guardian_ids to non-missing birth dates\n",
    "mapping_dict = df.dropna(subset=['guardian_birth_date']).set_index('guardian_id')['guardian_birth_date'].to_dict()\n",
    "\n",
    "# Fill missing birth dates using the mapping dictionary\n",
    "df['guardian_birth_date'] = df['guardian_id'].map(mapping_dict).fillna(df['guardian_birth_date'])\n",
    "\n",
    "# Filter rows where guardian_birth_date is out of bounds\n",
    "out_of_bounds_dates = df[(df['guardian_birth_date'].dt.year < 1800) | (df['guardian_birth_date'].dt.year > 2100)]\n",
    "\n",
    "# Remove rows with out-of-bounds dates\n",
    "df.drop(out_of_bounds_dates.index, inplace=True)\n",
    "\n",
    "\n",
    "# Step 3: Calculate the age at the time of the interview\n",
    "df['guardian_age'] = pd.NaT  # NEW VARIABLE: Initialize the column with missing values\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['guardian_birth_date']).days // 365\n",
    "        df.at[i, 'guardian_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "## Note: certain date of births are observed to be precisely 100 years off, \n",
    "        #try adding or subtracting 100 years to see if the age then becomes valid\n",
    "# Step 4: if guardian_age is below 18 or above 99, see if subtracting or adding 100 years will correct the \n",
    "    # 'guardian\"age' (making it between 18 and 99). \n",
    "    # if it does, update 'guardian_birth_date' and 'guardian_age' with the corresponding modification.\n",
    "# Filter rows where guardian_age is below 18 or above 99\n",
    "filtered_df = df[(df['guardian_age'] < 18) | (df['guardian_age'] > 99)]\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in filtered_df.iterrows():\n",
    "    birth_date = row['guardian_birth_date']\n",
    "    original_age = row['guardian_age']\n",
    "\n",
    "    # Subtract or add 100 years to correct age\n",
    "    modified_birth_date_minus = birth_date - timedelta(days=36525)  # Subtracting 100 years\n",
    "    modified_birth_date_plus = birth_date + timedelta(days=36525)  # Adding 100 years\n",
    "\n",
    "    modified_age_minus = df.at[index, 'guardian_age'] - 100\n",
    "    modified_age_plus = df.at[index, 'guardian_age'] + 100\n",
    "\n",
    "    # Check if the modified ages fall within a reasonable and valid age range\n",
    "    if 18 <= modified_age_minus <= 65:\n",
    "        df.at[index, 'guardian_birth_date'] = modified_birth_date_minus\n",
    "        df.at[index, 'guardian_age'] = modified_age_minus\n",
    "    elif 18 <= modified_age_plus <= 65:\n",
    "        df.at[index, 'guardian_birth_date'] = modified_birth_date_plus\n",
    "        df.at[index, 'guardian_age'] = modified_age_plus\n",
    "\n",
    "# Step 5: Clear 'guardian_age' and 'guardian_birth_date' where 'guardian_age' \n",
    "        #values are less than 18 or above 99 -- dont remove row, clear field\n",
    "filtered_rows = df[(df['guardian_age'] < 18) | (df['guardian_age'] > 99)]\n",
    "\n",
    "# Update 'guardian_age' and 'guardian_birth_date' with null/missing values\n",
    "df.loc[filtered_rows.index, ['guardian_age', 'guardian_birth_date']] = None\n",
    "\n",
    "\n",
    "# Last step: Write the updated dataframe to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3945f9d-a170-4ad9-b4c5-1b30c35898bd",
   "metadata": {},
   "source": [
    "## Guardian Sex\n",
    "\n",
    "Here, we use the gender guesser to fill in missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6879ab75-8f49-4803-9bfa-3a5a31f5b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize gender values for more proper analysis\n",
    "df['guardian_sex'] = df['guardian_sex'].replace('F', 'Female')\n",
    "df['guardian_sex'] = df['guardian_sex'].replace('M', 'Male')\n",
    "df['guardian_sex'] = df['guardian_sex'].replace('Unknown', '')\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711cca22-b544-41b0-9496-36ee8a083807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both the \"gender_guesser\" and \"Genderize\" library to fill in missing values\n",
    "# warning: Genderize relies on HTTP requests, so it may lag\n",
    "\n",
    "# Create a gender detector\n",
    "detector = gender.Detector()\n",
    "\n",
    "# Function to fill missing gender\n",
    "def fill_missing_gender(row):\n",
    "    # First try to fill in from existing data in different rows\n",
    "    if row['guardian_id'] in guardian_sex_map:\n",
    "        return guardian_sex_map[row['guardian_id']]\n",
    "    \n",
    "    # Secondary method - guess gender using gender_guesser\n",
    "    name = row['guardian'].split()[0]  # assuming first name is the first part\n",
    "    guessed_gender = detector.get_gender(name)\n",
    "    \n",
    "    # Map guessed gender to your gender categories\n",
    "    if guessed_gender in ['male', 'mostly_male']:\n",
    "        return 'Male'\n",
    "    elif guessed_gender in ['female', 'mostly_female']:\n",
    "        return 'Female'\n",
    "    \n",
    "    # Tertiary method - use genderize.io\n",
    "    else:\n",
    "        guessed_gender = Genderize().get([name])[0]['gender']\n",
    "        if guessed_gender == 'male':\n",
    "            return 'Male'\n",
    "        elif guessed_gender == 'female':\n",
    "            return 'Female'\n",
    "        else:\n",
    "            return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9680dbf4-ac28-4897-a3e2-174f71d51789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN MISSING VALUES: Guess the gender using customized function\n",
    "# Creating a mapping from guardian_id to guardian_sex\n",
    "guardian_sex_map = df.dropna(subset=['guardian_sex']).groupby('guardian_id')['guardian_sex'].first().to_dict()\n",
    "\n",
    "# Select rows where guardian_sex is missing\n",
    "missing_gender_indices = df[df['guardian_sex'].isna()].index\n",
    "\n",
    "# Apply the function only to rows with missing guardian_sex\n",
    "df.loc[missing_gender_indices, 'guardian_sex'] = df.loc[missing_gender_indices].apply(fill_missing_gender, axis=1)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab4ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using direct mapping to create dummy variable out of guardian_sex\n",
    "df['guardian_female'] = np.where(df['guardian_sex'] == 'Female', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bb666-c052-4c9a-a373-7a2638ddf708",
   "metadata": {},
   "source": [
    "## Language of Interview\n",
    "\n",
    "There are only less than 50 missing or invalid variables. As most interviews were done in the english language, impute variable with \"English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53330583-3c56-4a9d-bcee-19727360cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume interview was done in english. Fill empty values with \"English\"\n",
    "df['language'].fillna(\"English\", inplace=True)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820c9b2-5124-4242-9cfd-7c0c568ff999",
   "metadata": {},
   "source": [
    "## Mode of Interview\n",
    "\n",
    "convert to dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c772ac3-3273-4e95-94a5-f2fafb0efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dummy variable\n",
    "# Using direct mapping to create dummy variable out of mode\n",
    "df['phone_interview'] = np.where(df['mode'] == 'Phone', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca48e3-011c-40fa-80af-e53e44b0d3b0",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d460a-4967-4f05-ae00-2384c66eb7ed",
   "metadata": {},
   "source": [
    "## Student Enrollment Date\n",
    "\n",
    "Erroneous years range from 1915 to 2121. need to fix them, and reference other columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e901115-b373-42f8-b4c7-fdedbe29d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'student_enrollment_date' to datetime format\n",
    "df['student_enrollment_date'] = pd.to_datetime(df['student_enrollment_date'], errors='coerce')\n",
    "\n",
    "#fix erroneous and missing years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db8661-9234-4a9e-8585-81f63f07fd08",
   "metadata": {},
   "source": [
    "## Student Disability Status\n",
    "\n",
    "*If entry is blank, change to 'None' if interview disregards questions intended for parents of students with disabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723b93ec-c71e-40f9-b34a-a980b54da879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert disability questions from string to numeric data type\n",
    "df['QD1'] = pd.to_numeric(df['QD1'], errors='coerce')\n",
    "df['QD1a'] = pd.to_numeric(df['QD1a'], errors='coerce')\n",
    "df['QD2'] = pd.to_numeric(df['QD2'], errors='coerce')\n",
    "df['QD2a'] = pd.to_numeric(df['QD2a'], errors='coerce')\n",
    "\n",
    "# Custom function to check if any of the disability question columns have value between 1 to 5\n",
    "def has_disability(row):\n",
    "    return 1 <= row['QD1'] <= 5 or 1 <= row['QD1a'] <= 5 or 1 <= row['QD2'] <= 5 or 1 <= row['QD2a'] <= 5\n",
    "\n",
    "# Apply the custom function and fill in the missing values in 'student_has_disability'\n",
    "df['student_has_disability'] = df.apply(lambda row: has_disability(row) if pd.isna(row['student_has_disability']) else row['student_has_disability'], axis=1)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6f308-0d54-4e57-a7df-9144cee7c5dd",
   "metadata": {},
   "source": [
    "## Student, Hispanic\n",
    "\n",
    "remove anything that is not \"yes\" or \"no\". reconcile \"dont know\" and \"refused\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd96b756-0128-4ce0-976d-9ed748f4de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove anything that is not \"yes\" or \"no\", and reconcile \"dont know\" and \"refused\" values\n",
    "\n",
    "# Update the guardian_hispanic column\n",
    "df.loc[~df[\"guardian_hispanic\"].isin([\"Yes\", \"No\"]), \"guardian_hispanic\"] = \"\"\n",
    "\n",
    "#check the column \"guardian_native_language\" \"guardian_hispanic\", \"student_native_language\"\n",
    "# Define a function to fill missing values in student_hispanic column based on conditions\n",
    "def fill_student_hispanic(row):\n",
    "    if pd.isnull(row['student_hispanic']):\n",
    "        if row['guardian_hispanic'] == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif row['guardian_hispanic'] == 'No':\n",
    "            return 'No'\n",
    "        elif row['guardian_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif row['student_native_language'] == 'Spanish':\n",
    "            return 'Yes'\n",
    "        elif pd.notnull(row['student_native_language']):\n",
    "            return 'No'\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return row['student_hispanic']\n",
    "\n",
    "# Apply the function to fill missing values in student_hispanic column\n",
    "df['student_hispanic'] = df.apply(fill_student_hispanic, axis=1)\n",
    "\n",
    "# Convert \"yes\" and \"no\" to binary dummy variables\n",
    "#df = pd.get_dummies(df, columns=[\"guardian_hispanic\"], prefix=\"guardian_hispanic\", drop_first=True)\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d3d44-d181-42c4-97ed-3b3373075b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if student_race is missing, consider using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d3621-4b56-45a5-8328-971053cb8350",
   "metadata": {},
   "source": [
    "## Student Birth Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3992624e-2eef-49a3-8936-f7fe0c932c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use student's DOB and interview year to determine student’s age during time of their guardian's interview. \n",
    "# CORRECT erroneous 'student_birth_date' and 'date' variable first before determining age\n",
    "\n",
    "# Step 1: Convert 'student_birth_date' and 'date' columns to datetime\n",
    "df['student_birth_date'] = pd.to_datetime(df['student_birth_date'], errors='coerce')\n",
    "# df['date'] = pd.to_datetime(df['date'], errors='coerce') <---if necessary\n",
    "\n",
    "# Step 2: try to map student_birth_date with corresponding student_id\n",
    "# Create a dictionary mapping student_id's to non-missing birth dates\n",
    "mapping_dict = df.dropna(subset=['student_birth_date']).set_index('student_id')['student_birth_date'].to_dict()\n",
    "\n",
    "# Fill missing birth dates using the mapping dictionary\n",
    "df['student_birth_date'] = df['student_id'].map(mapping_dict).fillna(df['student_birth_date'])\n",
    "\n",
    "#Step 3:  Filter rows where guardian_birth_date is out of bounds\n",
    "out_of_bounds_dates = df[(df['student_birth_date'].dt.year < 1800) | (df['student_birth_date'].dt.year > 2100)]\n",
    "\n",
    "# Remove rows with out-of-bounds dates\n",
    "df.drop(out_of_bounds_dates.index, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Calculate the age at the time of their guardian's interview\n",
    "df['student_age'] = pd.NaT  # NEW VARIABLE: Initialize the column with missing values\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        age = (row['date'] - row['student_birth_date']).days // 365\n",
    "        df.at[i, 'student_age'] = age\n",
    "    except:\n",
    "        pass  # Ignore any dates that are out of bounds\n",
    "\n",
    "## Note: certain date of births are observed to be precisely 100 years off, \n",
    "        #try adding or subtracting 100 years to see if the age then becomes valid\n",
    "# Step 5: if student is below 0 or above 6, see if subtracting or adding 100 years will correct the \n",
    "    # 'student \"age' (making it between 0 and 6). \n",
    "    # if it does, update 'student_birth_date' and 'student_age' with the corresponding modification.\n",
    "# Filter rows where student_age is below 0 or above 6\n",
    "filtered_df = df[(df['student_age'] < 0) | (df['student_age'] > 6)]\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in filtered_df.iterrows():\n",
    "    birth_date = row['student_birth_date']\n",
    "    original_age = row['student_age']\n",
    "\n",
    "    # Subtract or add 100 years to correct age\n",
    "    modified_birth_date_minus = birth_date.replace(year=birth_date.year - 100)  # Subtracting 100 years\n",
    "    modified_birth_date_plus = birth_date.replace(year=birth_date.year + 100)  # Adding 100 years\n",
    "\n",
    "    modified_age_minus = df.at[index, 'student_age'] - 100\n",
    "    modified_age_plus = df.at[index, 'student_age'] + 100\n",
    "\n",
    "    # Check if the modified ages fall within a reasonable and valid age range\n",
    "    if 0 <= modified_age_minus <= 6:\n",
    "        df.at[index, 'student_birth_date'] = modified_birth_date_minus\n",
    "        df.at[index, 'student_age'] = modified_age_minus\n",
    "    elif 0 <= modified_age_plus <= 6:\n",
    "        df.at[index, 'student_birth_date'] = modified_birth_date_plus\n",
    "        df.at[index, 'student_age'] = modified_age_plus\n",
    "\n",
    "# Step 6: Clear 'student_age' and 'student_birth_date' where 'student_age' \n",
    "        #values are less than 0 or above 6 -- dont remove row, clear field\n",
    "filtered_rows = df[(df['student_age'] < 0) | (df['student_age'] > 6)]\n",
    "\n",
    "# Update 'student-age' and 'student_birth_date' with null/missing values\n",
    "df.loc[filtered_rows.index, ['student_age', 'student_birth_date']] = None\n",
    "\n",
    "# Step 7: also calculate the student's age in months\n",
    "# Calculate age in months using a reference date\n",
    "df['student_age_months'] = ((df['date'] - df['student_birth_date']) / pd.Timedelta(days=30.44))\n",
    "df['student_age_months'] = df['student_age_months'].where(df['student_age_months'].notnull() & np.isfinite(df['student_age_months']), np.nan).astype(float)\n",
    "\n",
    "# Last step: Write the updated dataframe to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b75e0-4753-4a15-8d2d-068d65477886",
   "metadata": {},
   "source": [
    "## Student in last year\n",
    "\n",
    "41% missing values, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9bd7fc8a-e7ca-4ad4-9ceb-993567a9ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN MISSING VALUES\n",
    "\n",
    "###########\n",
    "# Reference variable 'Q1', which is a question intended for first-timers. if values are 1 and above, i.e. answered, assume that student_in_last_year equals to false.\n",
    "# Convert 'Q1' to numeric, invalid values will be set as NaN\n",
    "df['Q1'] = pd.to_numeric(df['Q1'], errors='coerce')\n",
    "\n",
    "# fill in missing values -> this effectively cuts missing values from 41% to 17%.\n",
    "df.loc[df['Q1'] > 0, 'student_in_last_year'] = False\n",
    "df.loc[df['Q1'] == -3, 'student_in_last_year'] = True # if question was skipped conditionally, it assumes student was enrolled the previous year\n",
    "\n",
    "###########\n",
    "# Fill missing values in 'student_in_last_year' based on matching 'student_id' and 'evaluation_start_year' -> effectively drops from 17% to 5.5%.\n",
    "df['student_in_last_year'] = df.groupby(['student_id', 'evaluation_start_year'])['student_in_last_year'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "\n",
    "###########\n",
    "# check if there are past entries for each student_id' where there is an entry in the previous evaluation year\n",
    "# Custom function to check if evaluation_start_year - 1 exists for the same student_id\n",
    "def check_previous_year(row):\n",
    "    if pd.isna(row['student_in_last_year']):\n",
    "        prev_year = row['evaluation_start_year'] - 1\n",
    "        exists = df[(df['student_id'] == row['student_id']) & (df['evaluation_start_year'] == prev_year)].shape[0] > 0\n",
    "        if exists:\n",
    "            return True\n",
    "    return row['student_in_last_year']\n",
    "\n",
    "# Apply the custom function to each row\n",
    "df['student_in_last_year'] = df.apply(check_previous_year, axis=1)\n",
    "\n",
    "# Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5fbc1-5ea4-4de7-ad32-6a3c9b995b47",
   "metadata": {},
   "source": [
    "## Student Native Language\n",
    "\n",
    "Apply similar steps as Guardian's native language for statistical purposes. Less than 5% of values are missing, and assuming native language by guardian's native language should not drastically affect analysis or introduce bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "130c134b-44c2-46cf-926d-8d167653f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "# Reference 'guardian_native_language' to fill in missing values\n",
    "df['student_native_language'] = df['student_native_language'].fillna(df['guardian_native_language'])\n",
    "\n",
    "# Afterwards, reference 'student_id' as well\n",
    "df['student_native_language'] = df['student_native_language'].fillna(df.groupby('student_id')['student_native_language'].transform('first'))\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d128d-897d-4cb8-9019-327e24bbe9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider grouping languages other than English and Spanish together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a46cb-a2b7-4d1b-91b4-c6f5ca63f617",
   "metadata": {},
   "source": [
    "## Student Program Type\n",
    "\n",
    "For missing values, we can check the center's program type. Apply modal value of assigned programif above 90% frequency threshold. Then, need to deal with variables that state both \"head start\" and \"early head start.\" Can possibly also base it off student age. 7% is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29373042-ccd0-45e4-8bf4-20c28360fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mode and check if its frequency is above 80%\n",
    "def modal_program_to_use(ser):\n",
    "    mode_count = ser.value_counts()\n",
    "    total = len(ser.dropna())\n",
    "    if total > 0 and (mode_count.iloc[0] / total) > 0.80:\n",
    "        return mode_count.index[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Compute the modes that satisfy the condition for each center\n",
    "modes_to_use_by_center = df.groupby('center')['student_program_type'].transform(modal_program_to_use)\n",
    "\n",
    "# Replace missing values in 'student_service_type' column based on center's mode\n",
    "df['student_program_type'].fillna(modes_to_use_by_center, inplace=True)\n",
    "\n",
    "# Next, compute the modes that satisfy the condition for each program\n",
    "modes_to_use_by_program = df.groupby('program')['student_program_type'].transform(modal_program_to_use)\n",
    "\n",
    "# Replace remaining missing values in 'student_service_type' column based on program's mode\n",
    "df['student_program_type'].fillna(modes_to_use_by_program, inplace=True)\n",
    "\n",
    "# Fill the remaining missing values with the most frequent category overall, for analytical purposes\n",
    "most_frequent_program = df['student_program_type'].mode().iloc[0]\n",
    "df['student_program_type'].fillna(most_frequent_program, inplace=True)\n",
    "\n",
    "# Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19ee73-5275-450c-bddd-98476604d5dc",
   "metadata": {},
   "source": [
    "## Student Race\n",
    "\n",
    "#there are too many combinations, over 85 of them. need to simplify and create dummy variables. If student_race is missing, consider using guardian variable (for statistical purposes). cannot assume individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db9e9314-05e3-4ccc-9306-6bf49a660bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## clean and normalize\n",
    "# Convert to lowercase\n",
    "# Remove leading and trailing whitespace\n",
    "# Remove values that start with ', '\n",
    "df['student_race'] = df['student_race'].str.lower().str.strip().str.replace('^, ', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4361cb4-0299-4870-a667-3ab2abdb8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simplify categories\n",
    "def simplify_race_category(category):\n",
    "    # Check for NaN (missing value)\n",
    "    if pd.isna(category):\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    # Convert category to lowercase for case-insensitive comparison\n",
    "    category = category.lower()\n",
    "    \n",
    "    # Count how many specific categories are in the value\n",
    "    count = 0\n",
    "    if \"white\" in category:\n",
    "        count += 1\n",
    "    if \"black\" in category or \"african american\" in category:\n",
    "        count += 1\n",
    "    if \"asian\" in category:\n",
    "        count += 1\n",
    "    if \"native\" in category or \"american indian\" in category or \"alaska native\" in category:\n",
    "        count += 1\n",
    "    if \"hawaiian\" in category or \"pacific islander\" in category:\n",
    "        count += 1\n",
    "    if \"other\" in category:\n",
    "        count += 1\n",
    "    if \"don't know\" in category or \"unknown\" in category:\n",
    "        count += 0\n",
    "    \n",
    "    # Based on count, return the simplified category\n",
    "    if count == 0:\n",
    "        return \"Unknown\"\n",
    "    elif count == 1:\n",
    "        if \"white\" in category:\n",
    "            return \"White\"\n",
    "        if \"black\" in category or \"african american\" in category:\n",
    "            return \"Black\"\n",
    "        if \"asian\" in category:\n",
    "            return \"Asian\"\n",
    "        if \"native\" in category or \"american indian\" in category or \"alaska native\" in category:\n",
    "            return \"Native American or Alaska Native\"\n",
    "        if \"hawaiian\" in category or \"pacific islander\" in category:\n",
    "            return \"Native Hawaiian or Pacific Islander\"\n",
    "        if \"other\" in category:\n",
    "            return \"Other\"\n",
    "    else:\n",
    "        return \"Multiracial\"\n",
    "\n",
    "# Apply the simplification function to the 'student_race' column\n",
    "df['student_race'] = df['student_race'].apply(simplify_race_category)\n",
    "\n",
    "# Group by student_id and fill missing values in student_race\n",
    "df['student_race'] = df.groupby('student_id')['student_race'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "196519e8-fcd0-467a-a8a8-49dd062e2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider filling unknown student race values to that of their guardian, (for statistical purposes)\n",
    "mask = (df['student_race'] == 'Unknown') & (df['guardian_race'] != 'Unknown')\n",
    "df.loc[mask, 'student_race'] = df.loc[mask, 'guardian_race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c160ddcf-d139-4f57-ae11-44a5ca2bc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb8856-0702-4e8c-9eed-aba58ec89b13",
   "metadata": {},
   "source": [
    "## Student Service Type\n",
    "\n",
    "Try to reference off missing variables like program or center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd7d7123-62bb-4265-9e7e-fcd1b495bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mode and check if its frequency is above 75%\n",
    "def mode_to_use(ser):\n",
    "    mode_count = ser.value_counts()\n",
    "    total = len(ser.dropna())\n",
    "    if total > 0 and (mode_count.iloc[0] / total) > 0.75:\n",
    "        return mode_count.index[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Compute the modes that satisfy the condition for each center\n",
    "modes_to_use_by_center = df.groupby('center')['student_service_type'].transform(mode_to_use)\n",
    "\n",
    "# Replace missing values in 'student_service_type' column based on center's mode\n",
    "df['student_service_type'].fillna(modes_to_use_by_center, inplace=True)\n",
    "\n",
    "# Next, compute the modes that satisfy the condition for each program\n",
    "modes_to_use_by_program = df.groupby('program')['student_service_type'].transform(mode_to_use)\n",
    "\n",
    "# Replace remaining missing values in 'student_service_type' column based on program's mode\n",
    "df['student_service_type'].fillna(modes_to_use_by_program, inplace=True)\n",
    "\n",
    "# Fill the remaining missing values with the most frequent category overall, for analytical purposes\n",
    "most_frequent_category = df['student_service_type'].mode().iloc[0]\n",
    "df['student_service_type'].fillna(most_frequent_category, inplace=True)\n",
    "\n",
    "# Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a8e69-c4e4-4d25-8c87-cc9ce23bd330",
   "metadata": {},
   "source": [
    "## Student Sex\n",
    "\n",
    "use gender_guesser again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07aa0338-cea9-4f6d-a53d-9b080b4ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_gender_student(row):\n",
    "    # First try to fill in from existing data in different rows\n",
    "    if row['student_id'] in student_sex_map:\n",
    "        return student_sex_map[row['student_id']]\n",
    "    \n",
    "    # Secondary method - guess gender using gender_guesser\n",
    "    name = row['student'].split()[0]  # assuming first name is the first part\n",
    "    guessed_gender = detector.get_gender(name)\n",
    "    \n",
    "    # Map guessed gender to your gender categories\n",
    "    if guessed_gender in ['male', 'mostly_male']:\n",
    "        return 'Male'\n",
    "    elif guessed_gender in ['female', 'mostly_female']:\n",
    "        return 'Female'\n",
    "    \n",
    "    # Tertiary method - use genderize.io\n",
    "    else:\n",
    "        guessed_gender = Genderize().get([name])[0]['gender']\n",
    "        if guessed_gender == 'male':\n",
    "            return 'Male'\n",
    "        elif guessed_gender == 'female':\n",
    "            return 'Female'\n",
    "        else:\n",
    "            return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f1a1eb3-899b-4d89-b5d6-1101a716df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize gender values for more proper analysis\n",
    "df['student_sex'] = df['student_sex'].replace('F', 'Female')\n",
    "df['student_sex'] = df['student_sex'].replace('M', 'Male')\n",
    "df['student_sex'] = df['student_sex'].replace('Unknown', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cb0da00-c8e2-4a59-9ad0-ab72550c41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN MISSING VALUES: Guess the gender using customized function\n",
    "# Creating a mapping from guardian_id to guardian_sex\n",
    "student_sex_map = df.dropna(subset=['student_sex']).groupby('student_id')['student_sex'].first().to_dict()\n",
    "\n",
    "# Select rows where guardian_sex is missing\n",
    "missing_gender_indices_student = df[df['student_sex'].isna()].index\n",
    "\n",
    "# Apply the function only to rows with missing guardian_sex\n",
    "df.loc[missing_gender_indices_student, 'student_sex'] = df.loc[missing_gender_indices_student].apply(fill_missing_gender_student, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f847e-8a01-4bae-bf1f-956cc75ee2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using direct mapping to create dummy variable out of guardian_sex\n",
    "df['student_female'] = np.where(df['student_sex'] == 'Female', True, False)\n",
    "\n",
    "#save updates to csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82471932-cc4a-45ec-8ee2-5a62ff5b2c9c",
   "metadata": {},
   "source": [
    "## Student Was Early Headstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91c4368a-4264-48dd-b630-c59d2b757a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of 'True' and 'False' to boolean\n",
    "df['student_was_early_head_start'] = df['student_was_early_head_start'].replace({'False': False, 'True': True})\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef1b7-0e2a-4ab1-a3b4-1771c58c7509",
   "metadata": {},
   "source": [
    "## Student Was Head Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e8dfe52-a0b2-4f0f-bf57-ff13298cd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of 'True' and 'False' to boolean\n",
    "df['student_was_head_start'] = df['student_was_head_start'].replace({'False': False, 'True': True})\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece5bc2-d63b-41d6-97b7-d63da2e8f1db",
   "metadata": {},
   "source": [
    "# Next Section: Likert Scale Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb678305-1a6e-4640-9a30-eb12157d53ed",
   "metadata": {},
   "source": [
    "### CHANGE ALL UNANSWERED VALUES TO BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1a6db2-7144-4713-a59e-bc2444e61ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##CHANGE ALL UNANSWERED VALUES TO BLANK##\n",
    "#determine all of the likert scale interview variables\n",
    "\n",
    "variables = ['Q1', 'Q1a', 'Q1b', 'Q1c', 'Q1d', 'Q2', 'Q2a', 'Q3', 'Q3a', 'Q4', 'Q4a', 'Q5', 'Q5a', 'Q6', 'Q6a',\n",
    "             'Q7', 'Q7a', 'Q8', 'Q8a', 'QD1', 'QD1a', 'QD2', 'QD2a', 'Q9', 'Q9a', 'Q10', 'Q10a', 'Q11', 'Q11a',\n",
    "             'Q12', 'Q12a', 'Q13', 'Q13a', 'Q14', 'Q14a', 'Q15', 'Q15a', 'Q16', 'Q16a', 'Q17', 'Q17a', 'Q18',\n",
    "             'Q18a', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25']\n",
    "\n",
    "#convert string values to numeric, change the datatype to int\n",
    "df[variables] = df[variables].apply(pd.to_numeric, errors='coerce', downcast='integer')\n",
    "\n",
    "#replace the row values with blanks if the value is below 0\n",
    "df[variables] = df[variables].applymap(lambda x: '' if x < 0 else x)\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad311e54-93f8-4baa-a8eb-49d1cf8059f5",
   "metadata": {},
   "source": [
    "### DROP ALL BLANK INTERVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae61458-8512-4a9b-9943-ae76dfdbe516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows deleted where all 'Q' variables are blank: 20470\n",
      "Percentage of rows deleted where all 'Q' variables are blank: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Select columns starting with 'Q'\n",
    "q_columns = [col for col in df.columns if col.startswith('Q')]\n",
    "\n",
    "# Count the number of rows where all 'Q' columns are blank or null\n",
    "count_all_blank = len(df[(df[q_columns].isnull() | (df[q_columns] == '')).all(axis=1)])\n",
    "\n",
    "# Calculate the percentage of rows where all 'Q' columns are blank\n",
    "percentage_all_blank = (count_all_blank / len(df)) * 100\n",
    "\n",
    "# Delete the rows where all 'Q' columns are blank\n",
    "df = df[~(df[q_columns].isnull() | (df[q_columns] == \"\")).all(axis=1)]\n",
    "\n",
    "# Print the number and percentage of deleted rows\n",
    "print(\"Number of rows deleted where all 'Q' variables are blank:\", count_all_blank)\n",
    "print(\"Percentage of rows deleted where all 'Q' variables are blank: {:.2f}%\".format(percentage_all_blank))\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f6bee-42ee-419a-a328-8537c1011fcd",
   "metadata": {},
   "source": [
    "## Imputation of unanswered responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd4e34-b514-459f-a2d1-a664f4f1ed8e",
   "metadata": {},
   "source": [
    "https://www.researchgate.net/post/How-to-handle-missing-values-in-a-likert-scale-type-survey\n",
    "\n",
    "You can handle this situation by applying following techniques.\n",
    "1. Neutral-value substitution: You can use neutral value of the likert scale. In your case neutral value would be 3.5 but if your overall average score is less than 3.5 then you could not able to use this method.\n",
    "2. Mean-value substitution: If the average response for your actual respondents is 2.67, then substitute 2.67 for all your missing values. If it’s 5, use 5. But be aware that it is applicable only when the number of respondents with missing data and the number of items missing were 20% or less. More here\n",
    "Article Missing Data in Likert Ratings: A Comparison of Replacement Methods\n",
    "3. Imputation: You can calculate those values by the help of SAS and SPSS by using Approximate Bayesian bootstrap with Propensity score. More here\n",
    "Article On the Imputation of Missing Data in Surveys with Likert-Type Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27ba4bcc-e989-4f27-8ae9-79301c5ccdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing values in 'Q8a': 529\n",
      "Percentage of missing values in 'Q8a': 3.88%\n"
     ]
    }
   ],
   "source": [
    "## need to consider how questions are intended to not be answered based on evaluation period and circumstances (e.g. student disability), filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ce3c9-41cf-49bb-92a8-fbbf0c27b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c9161-2398-4e46-8cc5-f852e68bc96d",
   "metadata": {},
   "source": [
    "# LIKERT SUM CALCULATION\n",
    "\n",
    "Calculate a likert sum for statistical analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd3fe70f-5e0d-4946-8bbe-94dbfa0c6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_44653/2388527200.py:7: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['likert_sum_initial'] = df[df['evaluation'] == 'Initial'][variable_questions_initial].sum(axis=1)\n",
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_44653/2388527200.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['likert_sum_mid_post'] = df[df['evaluation'].isin(['Midyear', 'Post'])][variable_questions_midpost].sum(axis=1)\n",
      "/var/folders/vw/1sthm_ts1dj_1hqg1xnsz19w0000gn/T/ipykernel_44653/2388527200.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['likert_sum_mid_post_initial'] = df[df['evaluation'].isin(['Midyear', 'Post'])][variable_questions_initial].sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "#likert sum for initial interviews\n",
    "#exclude Q10-Q11 due to non-response rate\n",
    "variable_questions_initial = ['Q2', 'Q3', 'Q4', 'Q5',\n",
    "                      'Q6', 'Q7', 'Q8', 'Q9', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18']\n",
    "\n",
    "# Calculate the Likert Sum for rows where evaluation is 'Initial'\n",
    "df['likert_sum_initial'] = df[df['evaluation'] == 'Initial'][variable_questions_initial].sum(axis=1)\n",
    "\n",
    "#likert sum for mid/post interviews\n",
    "#exclude Q10-Q11 because of non-response rate\n",
    "variable_questions_midpost = ['Q2', 'Q2a', 'Q3', 'Q3a', 'Q4', 'Q4a', 'Q5', 'Q5a',\n",
    "                      'Q6', 'Q6a', 'Q7', 'Q7a', 'Q8', 'Q8a', 'Q9', 'Q9a',\n",
    "                      'Q12', 'Q12a', 'Q13', 'Q13a',\n",
    "                      'Q14', 'Q14a', 'Q15', 'Q15a', 'Q16', 'Q16a', 'Q17', 'Q17a',\n",
    "                      'Q18', 'Q18a', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25']\n",
    "\n",
    "# Calculate the Likert Sum of mid/post questions for rows where evaluations are 'Midyear' and 'Post'\n",
    "df['likert_sum_mid_post'] = df[df['evaluation'].isin(['Midyear', 'Post'])][variable_questions_midpost].sum(axis=1)\n",
    "\n",
    "# Calculate the Likert Sum for initial questions for rows where evaluations are 'Midyear' and 'Post'\n",
    "df['likert_sum_mid_post_initial'] = df[df['evaluation'].isin(['Midyear', 'Post'])][variable_questions_initial].sum(axis=1)\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43ba21-fb9f-4cd4-9e92-3cb0b699f09c",
   "metadata": {},
   "source": [
    "## Survey Completion Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97e86c1e-bff8-4562-bcbd-cd8bd2106870",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Survey completion rate\n",
    "# Function to calculate completion rate as a proportion\n",
    "def calculate_completion_rate_as_proportion(row):\n",
    "    if row['evaluation'] == 'Initial':\n",
    "        variable_questions = variable_questions_initial\n",
    "    elif row['evaluation'] in ['Midyear', 'Post']:\n",
    "        variable_questions = variable_questions_midpost\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    completed_count = row[variable_questions].notnull().sum()\n",
    "    total_count = len(variable_questions)\n",
    "    \n",
    "    return completed_count / total_count\n",
    "\n",
    "# Apply the function to each row\n",
    "df['completion_rate'] = df.apply(calculate_completion_rate_as_proportion, axis=1)\n",
    "\n",
    "#Copy updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651e0a0-12f1-4056-9362-d663e07e7da8",
   "metadata": {},
   "source": [
    "## Calculate Normalized Likert Sum Difference Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6279c-22f0-417d-b643-47474cefcc2e",
   "metadata": {},
   "source": [
    "# Next Section: Open Interview Questions\n",
    "\n",
    "perhaps we can use data analysis to see how sentiments change\n",
    "https://www.surveypractice.org/article/25699-what-to-do-with-all-those-open-ended-responses-data-visualization-techniques-for-survey-researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8da15bc-743b-4ac1-b0a3-71c64338d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first need to translate spanish-language to english\n",
    "columns_to_translate = ['OQ1', 'OQ2', 'OQ3', 'OQ3a', 'OQ4', 'OQ5', 'OQ6', 'OQ7', 'OQ8', 'OQ9', 'OQ10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50ecf159-4630-4cbf-9766-fc3d13f12643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         YES I DO THE STORY THAT I HAVE SINCE MY OLD CH...\n",
      "1                                                       nan\n",
      "2         I am more open with telling the staff any prob...\n",
      "3         it has helped me and my children . I have lear...\n",
      "4         I mean my relationship with Stacy , its a good...\n",
      "                                ...                        \n",
      "204670    This is our last summer with the program .   W...\n",
      "204671    Mantuvo conversaciones , escucho acerca de nue...\n",
      "204672    La maestra hizo preguntas acerca de mi familia...\n",
      "204673    Se intereso por mi familia y nuestras necesida...\n",
      "204674    Manteniendo conversaciones , haciendo pregunta...\n",
      "Name: OQ1_cleaned, Length: 204675, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#SCRUB ALL NAMES WITH GENERIC NAMES\n",
    "# Load the spaCy multilingual language model\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# Define the generic placeholder for student names\n",
    "generic_name = \"Student\"\n",
    "\n",
    "# Define a function to remove names from a text string\n",
    "def remove_names(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = ' '.join([token.text if token.ent_type_ != 1 else generic_name for token in doc])\n",
    "    return cleaned_text\n",
    "\n",
    "# Convert float values to strings in the 'OQ1' column\n",
    "df['OQ1'] = df['OQ1'].astype(str)\n",
    "\n",
    "# Apply the function to the 'OQ1' column\n",
    "df['OQ1_cleaned'] = df['OQ1'].apply(remove_names)\n",
    "\n",
    "# Print the updated column\n",
    "print(df['OQ1_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10a045-101d-4eac-b9b7-5ab05cc725c1",
   "metadata": {},
   "source": [
    "## Interview Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed86eab9-d6bf-4dee-8277-6121a18d4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower() # converts all interview responses to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # removes special characters and punctuation\n",
    "    text = text.strip() # normalizes text e.g. extra spaces\n",
    "    # Apply additional normalization steps as needed\n",
    "    return text\n",
    "\n",
    "for column in df.columns:\n",
    "    if column.startswith('OQ'):\n",
    "        df[column] = df[column].apply(normalize_text)\n",
    "        \n",
    "\n",
    "#TOKENIZATION\n",
    "#REMOVING STOPWORDS\n",
    "#CORRECTING MISSPELLED WORDS\n",
    "#STEMMING AND LEMATIZATION\n",
    "#HANDLE MISSING VALUES\n",
    "\n",
    "###clear interview responses where text says \"refused to answer\" \"don't know\" or \"parent refused interview\"\n",
    "\n",
    "# Write the updated dataframe to .csv file\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39607fda-6bc8-4493-a872-cd304150ac36",
   "metadata": {},
   "source": [
    "# Remove Unnecessary Columns\n",
    "\n",
    "After meticulously going through the data cleaning process, it is important to ensure that the dataset is streamlined and efficient for analysis. Removing excess columns that do not contribute to the analysis or could introduce noise is a critical step in data preparation. This step not only enhances the readability and manageability of the dataset but also optimizes memory usage and potentially speeds up processing time. It’s vital to scrutinize each column and ascertain its relevance in context to the analysis goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "573f5a74-e6d5-4ad7-adc1-cbfe26822685",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE IMPRACTICAL, UNNECESSARY, PRIVATE, AND NO LONGER NEEDED COLUMNS\n",
    "# List of columns to be removed\n",
    "columns_to_remove = ['created_at', 'guardian', 'guardian_employment', 'guardian_highest_education', 'guardian_birth_date', 'evaluation_year',\n",
    "                     'guardian_sex', 'guardian_vendor_id', 'interview_id', 'interviewer_id', 'mode', 'student_sex',\n",
    "                     'student_was_early_head_start', 'student_was_head_start', # remove these two, high percent missing values\n",
    "                     'interviewer', 'interviewer_vendor_id', 'student', 'student_staff_vendor_id', 'student_birth_date',\n",
    "                     'student_vendor_id', 'student_staff', 'student_staff_id']\n",
    "\n",
    "# Removing the columns from the DataFrame\n",
    "df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "#save updates to working csv\n",
    "df.to_csv('../data/intv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb83f76-53e3-4a61-ad2e-ba6bdfa8a8a4",
   "metadata": {},
   "source": [
    "# Data Type Reconfiguration\n",
    "\n",
    "To make the analysis faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "400611cb-c46c-4f7b-b1e2-5568d0923ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer and assign data types based on majority or inferred data types\n",
    "df_inferred = df.infer_objects()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
